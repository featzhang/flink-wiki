<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>guadazi-flink-wiki</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><div><strong aria-hidden="true">1.</strong> Flink 架构</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="Flink-runtime.html"><strong aria-hidden="true">1.1.</strong> Flink runtime</a></li><li class="chapter-item expanded "><a href="Flink-on-yarn.html"><strong aria-hidden="true">1.2.</strong> Flink-on-yarn</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.3.</strong> task chain</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.4.</strong> sharing slot group</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.5.</strong> task slot</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.6.</strong> CoLocationGroup</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.7.</strong> SlotSharingGroup</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.8.</strong> Flink state backend</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.9.</strong> Restart Strategies(重启策略)</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.10.</strong> Flink 2PC</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.11.</strong> Flink checkpoint与savepoint</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.12.</strong> Flink 反压</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.13.</strong> Flink 状态管理与TTL</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.14.</strong> Flink time</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.15.</strong> Flink timer</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.16.</strong> Flink window</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.17.</strong> Flink watermark</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.18.</strong> Flink-slot-sharing-group</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.19.</strong> Flink内存管理</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> Flink API</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="Flink-sink.html"><strong aria-hidden="true">2.1.</strong> Flink三种Sink模式: Append、Upsert和Retract</a></li><li class="chapter-item expanded "><a href="Flink-AsyncIO.html"><strong aria-hidden="true">2.2.</strong> Flink AsyncIO</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.3.</strong> Flink join: broadcast join、interval join、window join、cogroup</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> Flink DataStream</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="Flink-join.html"><strong aria-hidden="true">3.1.</strong> Flink join</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Flink SQL</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="Flink-SQL-Join.html"><strong aria-hidden="true">4.1.</strong> Flink SQL join</a></li><li class="chapter-item expanded "><a href="Flink-SQL-Window.html"><strong aria-hidden="true">4.2.</strong> Flink window &amp; demo</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.3.</strong> Flink interval join</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.4.</strong> Flink temporal join</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.5.</strong> Flink broadcast join</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.6.</strong> Flink SQL join的状态管理</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.7.</strong> Flink SQL-window aggregate与group aggregate</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.8.</strong> Flink over window与TopN</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Flink SQL 原理</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="Flink-SQL-Calcite.html"><strong aria-hidden="true">5.1.</strong> Calcite原理</a></li><li class="chapter-item expanded "><a href="Flink-SQL-codeGen.html"><strong aria-hidden="true">5.2.</strong> Flink SQL代码生成</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.3.</strong> Flink SQL原理与blink优化</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> Flink metric</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">6.1.</strong> Flink累加器和计数器:Accumulators &amp; Counters</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.</strong> Flink CEP</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">8.</strong> Flink Connectors</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">8.1.</strong> kudu connector</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.</strong> Flink devops</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">9.1.</strong> Flink-UI</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.2.</strong> Yarn、JM、TM配置与参数</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.3.</strong> 常见异常与解决方案</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.</strong> Flink实践</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">10.1.</strong> interval join实现outjoin</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.2.</strong> 双流join，一条流延迟</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.3.</strong> keyedState扩容</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.4.</strong> Flink-bloomfilter近似去重</div></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">guadazi-flink-wiki</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <hr />
<p>title: &quot;Flink runtime&quot;
layout: post
date: 2021-01-15 19:47:00
category: bigdata
tags:</p>
<ul>
<li>Flink</li>
</ul>
<h2>share: false
comments: true</h2>
<h1 id="flink-runtime运行时"><a class="header" href="#flink-runtime运行时">Flink-runtime运行时</a></h1>
<h2 id="概述"><a class="header" href="#概述">概述</a></h2>
<p>其实这两个概念我们可以看作：资源共享链与资源共享组。当我们编写完一个Flink程序，从Client开始执行——&gt;JobManager——&gt;TaskManager——&gt;Slot启动并执行Task的过程中，会对我们提交的执行计划进行优化，其中有两个比较重要的优化过程是：任务链与处理槽共享组，前者是对执行效率的优化，后者是对内存资源的优化。</p>
<h3 id="graph优化"><a class="header" href="#graph优化">Graph优化</a></h3>
<p>在<code>StreamGraph</code>转换为JobGraph过程中，关键在于将多个 <code>StreamNode</code> 优化为一个 <code>JobVertex</code>，对应的 StreamEdge 则转化为 <code>JobEdge</code>，并且 <code>JobVertex</code> 和 <code>JobEdge</code> 之间通过 <code>IntermediateDataSet</code> （中间数据集）形成一个生产者和消费者的连接关系。每个<code>JobVertex</code>就是<code>JobManger</code>的一个任务调度单位（任务Task）。为了避免在这个过程中将关联性很强的几个<code>StreamNode</code>（算子）放到不同<code>JobVertex</code>（<code>Task</code>）中，从而导致因为<code>Task</code>执行产生的效率问题（数据交换（网络传输）、线程上下文切换），Flink会在<code>StreamGraph</code>转换为<code>JobGraph</code>过程中将可以优化的算子合并为一个算子链（也就是形成一个Task）。这样就可以把这条链上的算子放到一个线程中去执行，这样就提高了任务执行效率。</p>
<h2 id="作业链task-chain"><a class="header" href="#作业链task-chain">作业链Task-chain</a></h2>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210423124550.png" alt="img" /></p>
<ul>
<li>
<p>Chain：Flink会尽可能地将多个operator链接（chain）在一起形成一个task pipline。每个task pipline在一个线程中执行</p>
</li>
<li>
<p>优点：<em>它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。</em></p>
</li>
</ul>
<p>StreamGraph转换为JobGraph过程中，实际上是逐条审查每一个StreamEdge和该SteamEdge两头连接的两个StreamNode的特性，来决定该StreamEdge两头的StreamNode是不是可以合并在一起形成算子链。这个判断过程flink给出了明确的规则，我们看一下StreamingJobGraphGenerator中的isChainable（）方法：</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210423124551.png" alt="img" /></p>
<p>该方法返回true时两个端点才可以合并到一起，根据源码我们可以得出形成作业链的规则如下：</p>
<ol>
<li>上下游的并行度一致（槽一致）</li>
<li>该节点必须要有上游节点跟下游节点；</li>
<li>下游StreamNode的输入StreamEdge只能有一个）</li>
<li>上下游节点都在同一个 slot group 中（下面会解释 slot group）</li>
<li>下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</li>
<li>上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</li>
<li>上下游算子之间没有数据shuffle (数据分区方式是 forward)</li>
<li>用户没有禁用 chain</li>
</ol>
<p>二、开启/禁用全局作业链</p>
<p>用户能够通过禁用全局作业链的操作来关闭整个Flink的作业链，但是这个操作会影响到这个作业的执行情况，除非我们非常清楚作业的执行过程，否则不建议这么做：StreamExecutionEnvironment.disableOperatorChaining()。全局作业链关闭之后，如果想创建对应Operator的作业链，可以使用startNewChain()方法：someStream.filter(...).map(...).startNewChain().map(...)。注意该方法只对当前操作符及之后的操作符有效，所以上述代码只对两个map进行链条绑定。</p>
<p>三、禁用局部作业链</p>
<p>如果我们只想对某个算子执行禁用作业链，只需调用disableChaining（）方法：someSteam.map().disableChaining().filter()，该方法只会禁用当前算子的链条（上述代码中就是map），对其他算子操作不产生影响。</p>
<h2 id="处理槽共享组出于某中目的将多个task放到同一个slot中执行"><a class="header" href="#处理槽共享组出于某中目的将多个task放到同一个slot中执行">处理槽共享组(出于某中目的将多个Task放到同一个slot中执行)</a></h2>
<p>一、Task Slot</p>
<p>TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 <em>Task Slot</em> 的概念，通过 <em>Task Slot</em> 来定义Flink 中的计算资源。solt 对TaskManager内存进行平均分配，每个solt内存都相同，加起来和等于TaskManager可用内存，但是仅仅对内存做了隔离，并没有对cpu进行隔离。将资源 slot 化意味着来自不同job的task不会为了内存而竞争，而是每个task都拥有一定数量的内存储备。</p>
<p>通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输。也能共享一些数据结构，一定程度上减少了每个task的消耗。</p>
<p>二、共享槽</p>
<p>一个TaskManager中至少有一个插槽slot，每个插槽均分内存并且之间是内存隔离的，但是共享CPU。算子根据计算复杂度可以分为资源密集型与非资源密集型算子（可以认为有的算子计算时内存需求大，有些算子内存需求小）。现在有这么个情况：某个Job下的Tasks中既有资源密集型Task（A），又有非资源密集型Task（B），他们被分到不同的slot上，这就会产生一个问题，有的slot内存使用率大，有的slot内存使用率小，这样就很不公平，内存没有得到充分的利用。所以我们可以采用一个方案：将A、B放到同一个slot当中。</p>
<p>默认情况下，Flink 允许subtasks共享slot，条件是它们都来自同一个Job的不同task的subtask。结果可能一个slot持有该job的整个pipeline。允许槽共享，会有以下两个方面的好处：</p>
<ul>
<li>flink计算一个job所需slot数量时，只需要确定所其最大并行度（前提，保持默认SlotSharingGroup），而不用计算每一个任务的并行度的总和；</li>
<li>能更好的利用资源，如果没有solt共享，那些资源需求不大的map子任务将和资源需求更大的window占用相同的资源。</li>
</ul>
<p>Flink相同资源组里的多个Task可以共享一个Slot资源槽。具体共享机制又分两种：</p>
<p><strong>1、CoLocationGroup:</strong> 强制将 subtasks 放到同一个 slot 中，是一种硬约束</p>
<ul>
<li>保证把JobVertices的第n个运行实例和其他相同组内的JobVertices第n个实例运作在相同的slot中（所有的并行度相同的subTasks运行在同一个slot ）；</li>
<li>主要用于迭代流(训练机?学习模型) ，用来保证迭代头与迭代尾的第i个subtask能被调度到同一个TaskManager上。</li>
</ul>
<p><strong>2、SlotSharingGroup:</strong> 允许不同的JobVertices的部署在相同的Slot中，但这是一种宽约束，只是尽量做到不能完全保证。</p>
<ul>
<li>SlotSharingGroup是Flink中用来实现slot共享的类，它尽可能地让subTasks共享一个slot；</li>
<li>保证同一个group的并行度相同的sub-tasks 共享同一个slots ；</li>
<li>算子的默认group为default（即默认一个job下的subtask都可以共享一个slot）</li>
<li>为了防止不合理的共享，用户可以强制指定operator的共享组，比如： someStream.filter(...).slotSharingGroup(&quot;group1&quot;)；就强制指定了filter的slot共享组为group1；</li>
<li>要想确定一个未做SlotSharingGroup设置的算子的group是什么，可以根据上游算子的 group 和自身是否设置 group共同确定；</li>
<li>适当设置可以减少每个slot运行的线程数，从而整体上减少机?的负载。</li>
</ul>
<p>【参考文献】</p>
<ol>
<li><a href="https://www.infoq.cn/article/RWTM9o0SHHV3Xr8o8giT">Apache Flink进阶一: Runtime核心机制剖析</a></li>
</ol>
<hr />
<p>title: &quot;Flink on yarn&quot;
layout: post
date: 2019-08-10 14:58:00
category: bigdata
tags:</p>
<ul>
<li>Java</li>
<li>Flink</li>
</ul>
<h2>share: true
comments: true</h2>
<h2 id="flink-on-yarn-任务提交"><a class="header" href="#flink-on-yarn-任务提交">Flink-on-yarn 任务提交</a></h2>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210422173135.png" alt="这里写图片描述" /></p>
<h2 id="flink-on-yarn部署"><a class="header" href="#flink-on-yarn部署">flink on yarn部署</a></h2>
<p>flink on yarn需要的组件与版本如下</p>
<ol>
<li>Zookeeper 3.4.9 用于做Flink的JobManager的HA服务</li>
<li>hadoop 2.7.2 搭建HDFS和Yarn</li>
<li>flink 1.3.2 或者 1.4.1版本（scala 2.11）</li>
</ol>
<p>Zookeeper, HDFS 和 Yarn 的组件的安装可以参照网上的教程。</p>
<p>在zookeeper，HDFS 和Yarn的组件的安装好的前提下，在客户机上提交Flink任务，具体流程如下：</p>
<ul>
<li>在启动Yarn-Session 之前， 设置好HADOOP_HOME,YARN_CONF_DIR ， HADOOP_CONF_DIR环境变量中三者的一个。如下所示， 根据具体的hadoop 路径来设置</li>
</ul>
<pre><code class="language-shell">   $ export HADOOP_HOME=/usr/local/hadoop-current
</code></pre>
<ul>
<li>配置flink 目录下的flink-conf.yaml, 如下所示</li>
</ul>
<pre><code class="language-yaml">jobmanager.rpc.address: localhost
jobmanager.rpc.port: 6123
jobmanager.heap.mb: 256
taskmanager.heap.mb: 512
taskmanager.numberOfTaskSlots: 1
taskmanager.memory.preallocate: false
parallelism.default: 1
jobmanager.web.port: 8081

# yarn
yarn.maximum-failed-containers: 99999

#akka config
akka.watch.heartbeat.interval: 5 s
akka.watch.heartbeat.pause: 20 s
akka.ask.timeout: 60 s
akka.framesize: 20971520b

#high-avaliability
high-availability: zookeeper
## 根据安装的zookeeper信息填写
high-availability.zookeeper.quorum: 10.141.61.226:2181,10.141.53.244:2181,10.141.18.219:2181
high-availability.zookeeper.path.root: /flink
## HA 信息存储到HDFS的目录，根据各自的Hdfs情况修改
high-availability.zookeeper.storageDir: hdfs://hdcluster/flink/recovery/

#checkpoint config
state.backend: rocksdb
## checkpoint到HDFS的目录 根据各自安装的HDFS情况修改
state.backend.fs.checkpointdir: hdfs://hdcluster/flink/checkpoint
## 对外checkpoint到HDFS的目录
state.checkpoints.dir: hdfs://hdcluster/flink/savepoint

#memory config
env.java.opts: -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+AlwaysPreTouch -server -XX:+HeapDumpOnOutOfMemoryError
yarn.heap-cutoff-ratio: 0.2
taskmanager.memory.off-heap: true

</code></pre>
<ul>
<li>提交Yarn-Session，切换到flink的bin 目录下,提交命令如下</li>
</ul>
<pre><code class="language-shell">   $ ./yarn-session.sh -n 2 -s 6 -jm 3072 -tm 6144 -nm test -d
</code></pre>
<p>启动yarn-session的参数解释如下</p>
<table><thead><tr><th>参数</th><th>参数解释</th><th>设置推荐</th></tr></thead><tbody>
<tr><td>-n(--container)</td><td>taskmanager的数量</td><td></td></tr>
<tr><td>-s(--slots)</td><td>用启动应用所需的slot数量/ -s 的值向上取整，有时可以多一些taskmanager，做冗余 每个taskmanager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1</td><td>6～10</td></tr>
<tr><td>-jm</td><td>jobmanager的内存（单位MB)</td><td>3072</td></tr>
<tr><td>-tm</td><td>每个taskmanager的内存（单位MB)</td><td>根据core 与内存的比例来设置，-s的值＊ （core与内存的比）来算</td></tr>
<tr><td>-nm</td><td>yarn 的appName(现在yarn的ui上的名字)｜</td><td></td></tr>
<tr><td>-d</td><td>后台执行</td><td></td></tr>
</tbody></table>
<ul>
<li>提交yarn－session 后，可以在yarn的ui上看到一个应用（应用有一个appId）, 切换到flink的bin目录下，提交flink 应用。命令如下</li>
</ul>
<pre><code class="language-shell"> $ ./flink -run file:///home/yarn/test.jar -a 1 -p 12 -yid appId -nm flink-test -d
</code></pre>
<p>启动flink 应用的参数解释如下</p>
<table><thead><tr><th>参数</th><th>参数解释</th></tr></thead><tbody>
<tr><td>-j</td><td>运行flink 应用的jar所在的目录</td></tr>
<tr><td>-a</td><td>运行flink 应用的主方法的参数</td></tr>
<tr><td>-p</td><td>运行flink应用的并行度</td></tr>
<tr><td>-c</td><td>运行flink应用的主类, 可以通过在打包设置主类</td></tr>
<tr><td>-nm</td><td>flink 应用名字，在flink-ui 上面展示</td></tr>
<tr><td>-d</td><td>后台执行</td></tr>
<tr><td>--fromsavepoint</td><td>flink 应用启动的状态恢复点</td></tr>
</tbody></table>
<ul>
<li>启动flink应用成功，即可在yarn ui 点击对应应用的ApplicationMaster链接,既可以查看flink-ui ，并查看flink 应用运行情况。</li>
</ul>
<p>注：在安装部署遇到任何问题，可以在小象问答，微信群以及私聊提出，我们一般会在晚上作答（由于白天要上班，作答不及时请谅解。）</p>
<hr />
<p>title: &quot;Flink-sink&quot;
layout: post
date: 2021-04-11 14:58:00
category: bigdata
tags:</p>
<ul>
<li>Sink</li>
<li>Flink</li>
</ul>
<h2>share: true
comments: true</h2>
<h2 id="sink的三种模式"><a class="header" href="#sink的三种模式">Sink的三种模式</a></h2>
<p><a href="http://www.whitewood.me/2020/02/26/Flink-Table-%E7%9A%84%E4%B8%89%E7%A7%8D-Sink-%E6%A8%A1%E5%BC%8F/">Flink table的三种sink模式</a></p>
<p>Sink有INSERT、UPDATE 和 DELETE 三类，Table的sink模式有append、upsert和retract三种</p>
<table><thead><tr><th>Sink模式</th><th>Insert</th><th>Update</th><th>Delete</th><th>支持的存储</th></tr></thead><tbody>
<tr><td>Append(追加)</td><td>支持</td><td>不支持</td><td>不支持</td><td></td></tr>
<tr><td>Upsert(重复时更新)</td><td>支持</td><td>支持</td><td>支持</td><td>KV: HBase JDBC</td></tr>
<tr><td>Retract(允许撤销)</td><td>支持</td><td>支持</td><td>支持</td><td></td></tr>
</tbody></table>
<h3 id="upsert模式与retract模式的区别"><a class="header" href="#upsert模式与retract模式的区别">Upsert模式与Retract模式的区别</a></h3>
<p>Upsert模式需要唯一的key来传递更新消息，外部连接器需要明确知道这个唯一key的属性</p>
<p>Upsert模式和Retract模式</p>
<p>消息: 都为(Boolean,Row)二元组, 第一个元素代表操作类型:</p>
<table><thead><tr><th>模式</th><th>操作类型</th><th>插入</th><th>更新</th><th>删除</th></tr></thead><tbody>
<tr><td>Upsert模式</td><td>true 为 UPSERT消息(不存在则INSERT, 存在则UPDATE)<br/> false 为 DELETE消息</td><td>upsert消息</td><td>upsert消息</td><td>delete消息</td></tr>
<tr><td>Retract模式</td><td>true为添加消息<br/>false为撤回消息</td><td>添加消息</td><td>已更新行(上一行)的撤回消息<br/>更新行(新行)的添加消息</td><td>撤回消息</td></tr>
</tbody></table>
<h3 id="append模式-窗口聚合中的应用"><a class="header" href="#append模式-窗口聚合中的应用">Append模式-窗口聚合中的应用</a></h3>
<p>在实时聚合统计中，聚合统计的结果输出是由 Trigger 决定的，而 Append-Only 则意味着对于每个窗口实例（Pane，窗格）Trigger 只能触发一次，则就导致无法在迟到数据到达时再刷新结果。</p>
<p>通常来说，我们可以给 Watermark 设置一个较大的延迟容忍阈值来避免这种刷新（再有迟到数据则丢弃），但代价是却会引入较大的延迟。</p>
<h3 id="upsert模式"><a class="header" href="#upsert模式">Upsert模式</a></h3>
<p>支持 Append-Only 的操作和在有主键的前提下的 Update 和 Delete 操作.</p>
<p><strong>重复时更新</strong></p>
<p>Upsert 模式依赖业务主键来实现输出结果的更新和删除，因此非常适合 KV 数据库，比如 HBase、JDBC 的 TableSink 都使用了这种方式。</p>
<p>Upsert 模式是目前来说比较实用的模式，因为大部分业务都会提供原子或复合类型的主键，而在支持 KV 的存储系统也非常多，但要注意的是不要变更主键，具体原因会在下一节谈到。</p>
<h3 id="retract模式"><a class="header" href="#retract模式">Retract模式</a></h3>
<p><strong>允许撤销</strong></p>
<p>举个例子，假设我们将电商订单按照承运快递公司进行分类计数，有如下的结果表。</p>
<table><thead><tr><th align="left">公司</th><th align="left">订单数</th></tr></thead><tbody>
<tr><td align="left">中通</td><td align="left">2</td></tr>
<tr><td align="left">圆通</td><td align="left">1</td></tr>
<tr><td align="left">顺丰</td><td align="left">3</td></tr>
</tbody></table>
<p>那么如果原本一单为中通的快递，后续更新为用顺丰发货，对于 Upsert 模式会产生 <code>(true, (顺丰, 4))</code> 这样一条 changelog，但中通的订单数没有被修正。相比之下，Retract 模式产出 <code>(false, (中通, 1))</code> 和 <code>(true, (顺丰, 1))</code> 两条数据，则可以正确地更新数据。</p>
<hr />
<p>title: &quot;Flink asyncIO&quot;
layout: post
date: 2021-01-15 19:47:00
category: bigdata
tags:</p>
<ul>
<li>Flink</li>
</ul>
<h2>share: false
comments: true</h2>
<h1 id="flink-asyncio"><a class="header" href="#flink-asyncio">Flink-asyncIO</a></h1>
<p><img src="_v_images/20201207210251979_867932479.png" alt="" /></p>
<p>阿里贡献给flink的，优点就不说了嘛，官网上都有，就是写库不会阻塞性能更好</p>
<p>然后来看一下， Flink 中异步io主要分为两种</p>
<p>　　一种是有序Ordered</p>
<p>　　一种是无序UNordered</p>
<p>主要区别是往下游output的顺序（注意这里顺序不是写库的顺序既然都异步了写库的顺序自然是无法保证的），有序的会按接收的顺序继续往下游output发送，无序就是谁先处理完谁就先往下游发送</p>
<p>两张图了解这两种模式的实现</p>
<p><img src="_v_images/20201207210251873_2096107056.png" alt="" /></p>
<p>有序：record数据会通过异步线程写库，Emitter是一个守护进程，会不停的拉取queue头部的数据，如果头部的数据异步写库完成，Emitter将头数据往下游发送，如果头元素还没有异步写库完成，柱塞 <img src="_v_images/20201207210251766_1309898873.png" alt="" /></p>
<p>无序：record数据会通过异步线程写库，这里有两个queue,一开始放在uncompleteedQueue，当哪个record异步写库成功后就直接放到completedQueue中，Emitter是一个守护进程，completedQueue只要有数据，会不停的拉取queue数据往下游发送 </p>
<p>可以看到原理还是很简单的，两句话就总结完了，就是利用queue和java的异步线程，现在来看下源码</p>
<p>这里AsyncIO在Flink中被设计成operator中的一种，自然去OneInputStreamOperator的实现类中去找</p>
<p>于是来看一下AsyncWaitOperator.java</p>
<p>　　<img src="_v_images/20201207210251559_1488153726.png" alt="" /></p>
<p>看到它的open方法（open方法会在taskmanager启动job的时候全部统一调用，可以翻一下以前的文章）</p>
<p>这里启动了一个守护线程Emitter,来看下线程具体做了什么</p>
<p><img src="_v_images/20201207210251351_312304785.png" alt="" /></p>
<p>1处拉取数据，2处就是常规的将拉取到的数据往下游emit，Emitter拉取数据，这里先不讲因为分为有序的和无序的</p>
<p>这里已经知道了这个Emitter的作用是循环的拉取数据往下游发送</p>
<p>回到AsyncWaitOperator.java在它的open方法初始化了Emitter,那它是如何处理接收到的数据的呢，看它的ProcessElement（）方法</p>
<p><img src="_v_images/20201207210251144_1378849400.png" alt="" /></p>
<pre><code>![](_v_images/20201207210250637_60325951.png)
</code></pre>
<p><img src="_v_images/20201207210250031_255901575.png" alt="" /></p>
<p>其实主要就是三个个方法</p>
<p>先是！！！将record封装成了一个包装类StreamRecordQueueEntry，主要是这个包装类的构造方法中,创建了一个CompleteableFuture(这个的complete方法其实会等到用户代码执行的时候用户自己决定什么时候完成）</p>
<p>1处主要就是讲元素加入到了对应的queue,这里也分为两种有序和无序的</p>
<p><img src="_v_images/20201207210249826_218236941.png" alt="" /></p>
<p>这里也先不讲这两种模式加入数据的区别</p>
<p>接着2处就是调用用户的代码了，来看看官网的异步io的例子</p>
<p><img src="_v_images/20201207210249221_1218191589.png" alt="" /></p>
<p>给了一个Future作为参数，用户自己起了一个线程（这里思考一下就知道了为什么要新起一个异步线程去执行，因为如果不起线程的话，那processElement方法就柱塞了，无法异步了）去写库读库等，然后调用了这个参数的complete方法（也就是前面那个包装类中的CompleteableFuture）并且传入了一个结果</p>
<p>看下complete方法源码</p>
<p><img src="_v_images/20201207210248614_1185315462.png" alt="" /></p>
<p>这个resultFuture是每个record的包装类StreamRecordQueueEntry的其中一个属性是一个CompletableFuture</p>
<p>那现在就清楚了，用户代码在自己新起的线程中当自己的逻辑执行完以后会使这个异步线程结束，并输入一个结果</p>
<p>那这个干嘛用的呢</p>
<p>最开始的图中看到有序和无序实现原理，有序用一个queue,无序用两个queue分别就对应了</p>
<p>OrderedStreamElementQueue类中</p>
<p><img src="_v_images/20201207210248409_107213650.png" alt="" /></p>
<p>UnorderedStreamElementQueue类中</p>
<p><img src="_v_images/20201207210248304_1712699416.png" alt="" /></p>
<p>回到前面有两个地方没有细讲，一是两种模式的Emitter是如何拉取数据的，二是两种模式下数据是如何加入OrderedStreamElementQueue的</p>
<p>有序模式：</p>
<p>1.先来看一下有序模式的，Emitter的数据拉取，和数据的加入</p>
<p>　　　　其tryPut（）方法</p>
<p>　　　　  <img src="_v_images/20201207210248099_682570540.png" alt="" /></p>
<p>　　　  <img src="_v_images/20201207210247594_534665308.png" alt="" /></p>
<p>　　　　<em>onComplete**方法</em></p>
<p>　　　　　　　<em><img src="_v_images/20201207210247288_2059500658.png" alt="" /></em></p>
<pre><code>   onCompleteHandler方法
</code></pre>
<p>　　　　  　<img src="_v_images/20201207210247082_286767362.png" alt="" />　</p>
<p>　　这里比较绕，先将接收的数据加入queue中，然后onComplete()中当上一个异步线程getFuture() 其实就是每个元素包装类里面的那个CompletableFuture,当他结束时（会在用户方法用户调用complete时结束）异步调用传入的对象的 accept方法，accept方法中调用了onCompleteHandler（）方法，onCompleteHandler方法中会判断queue是否为空，以及queue的头元素是否完成了用户的异步方法，当完成的时候，就会将headIsCompleted这个对象signalAll（）唤醒</p>
<p>2.接着看有序模式Emitter的拉取数据</p>
<p><img src="_v_images/20201207210246482_1132368562.png" alt="" /></p>
<p>这里有序方式拉取数据的逻辑很清晰，如果为空或者头元素没有完成用户的异步方法，headIsCompleted这个对象会wait住（上面可以知道，当加入元素的到queue且头元素完成异步方法的时候会signalAll（））然后将头数据返回，往下游发送</p>
<p>这样就实现了有序发送，因为Emitter只拉取头元素且已经完成用户异步方法的头元素</p>
<p>无序模式： </p>
<p>　　这里和有序模式就大同小异了，只是变成了,接收数据后直接加入uncompletedQueue，当数据完成异步方法的时候就，放到completedQueue里面去并signalAll（），只要completedqueue里面有数据，Emitter就拉取往下发</p>
<p>这样就实现了无序模式，也就是异步写入谁先处理完就直接放到完成队列里面去，然后往下发，不用管接收数据的顺序</p>
<h1 id="flink-join"><a class="header" href="#flink-join">Flink join</a></h1>
<p>Flink DataStream API为用户提供了3个算子来实现双流join，分别是：</p>
<ul>
<li>join():   inner join，on window</li>
<li>coGroup():   custom join, on window</li>
<li>intervalJoin():   inner join, on time range, keyed stream</li>
</ul>
<p>另外，还提供了broadcast join来关联较小的</p>
<h2 id="准备数据"><a class="header" href="#准备数据">准备数据</a></h2>
<p>从Kafka分别接入点击流和订单流，并转化为POJO。</p>
<pre><code class="language-java">DataStream&lt;String&gt; clickSourceStream = env
  .addSource(new FlinkKafkaConsumer011&lt;&gt;(
    &quot;ods_analytics_access_log&quot;,
    new SimpleStringSchema(),
    kafkaProps
  ).setStartFromLatest());
DataStream&lt;String&gt; orderSourceStream = env
  .addSource(new FlinkKafkaConsumer011&lt;&gt;(
    &quot;ods_ms_order_done&quot;,
    new SimpleStringSchema(),
    kafkaProps
  ).setStartFromLatest());

DataStream&lt;AnalyticsAccessLogRecord&gt; clickRecordStream = clickSourceStream
  .map(message -&gt; JSON.parseObject(message, AnalyticsAccessLogRecord.class));
DataStream&lt;OrderDoneLogRecord&gt; orderRecordStream = orderSourceStream
  .map(message -&gt; JSON.parseObject(message, OrderDoneLogRecord.class));
</code></pre>
<h2 id="join"><a class="header" href="#join">join()</a></h2>
<p>join()算子提供的语义为&quot;<strong>Window join</strong>&quot;，即按照指定字段和（滚动/滑动/会话）窗口进行<strong>inner join</strong>，支持处理时间和事件时间两种时间特征。</p>
<p>以下示例以10秒滚动窗口，将两个流通过商品ID关联，取得订单流中的售价相关字段。</p>
<p><img src="//upload-images.jianshu.io/upload_images/195230-05b05dd1da1e6025.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" alt="" /></p>
<pre><code class="language-java">clickRecordStream
  .join(orderRecordStream)
  .where(record -&gt; record.getMerchandiseId())
  .equalTo(record -&gt; record.getMerchandiseId())
  .window(TumblingProcessingTimeWindows.of(Time.seconds(10)))
  .apply(new JoinFunction&lt;AnalyticsAccessLogRecord, OrderDoneLogRecord, String&gt;() {
    @Override
    public String join(AnalyticsAccessLogRecord accessRecord, OrderDoneLogRecord orderRecord) throws Exception {
      return StringUtils.join(Arrays.asList(
        accessRecord.getMerchandiseId(),
        orderRecord.getPrice(),
        orderRecord.getCouponMoney(),
        orderRecord.getRebateAmount()
      ), '\t');
    }
  })
  .print().setParallelism(1);
</code></pre>
<p>简单易用。</p>
<h2 id="cogroup"><a class="header" href="#cogroup">coGroup()</a></h2>
<p>只有inner join肯定还不够，如何实现left/right outer join呢？答案就是利用coGroup()算子。它的调用方式类似于join()算子，也需要开窗，但是CoGroupFunction比JoinFunction更加灵活，可以按照用户指定的逻辑匹配左流和/或右流的数据并输出。</p>
<p>以下的例子就实现了点击流left join订单流的功能，是很朴素的nested loop join思想（二重循环）。</p>
<pre><code class="language-java">clickRecordStream
  .coGroup(orderRecordStream)
  .where(record -&gt; record.getMerchandiseId())
  .equalTo(record -&gt; record.getMerchandiseId())
  .window(TumblingProcessingTimeWindows.of(Time.seconds(10)))
  .apply(new CoGroupFunction&lt;AnalyticsAccessLogRecord, OrderDoneLogRecord, Tuple2&lt;String, Long&gt;&gt;() {
    @Override
    public void coGroup(Iterable&lt;AnalyticsAccessLogRecord&gt; accessRecords, Iterable&lt;OrderDoneLogRecord&gt; orderRecords, Collector&lt;Tuple2&lt;String, Long&gt;&gt; collector) throws Exception {
      for (AnalyticsAccessLogRecord accessRecord : accessRecords) {
        boolean isMatched = false;
        for (OrderDoneLogRecord orderRecord : orderRecords) {
          // 右流中有对应的记录
          collector.collect(new Tuple2&lt;&gt;(accessRecord.getMerchandiseName(), orderRecord.getPrice()));
          isMatched = true;
        }
        if (!isMatched) {
          // 右流中没有对应的记录
          collector.collect(new Tuple2&lt;&gt;(accessRecord.getMerchandiseName(), null));
        }
      }
    }
  })
  .print().setParallelism(1);
</code></pre>
<p>CoGroupFunction中会返回所有数据，不管有没有匹配上</p>
<pre><code class="language-java">DataStream&lt;Tuple3&lt;Long, String, String&gt;&gt; input1 = ...;
input1 = input1.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, String&gt;&gt;() {

    @Override
    public long extractAscendingTimestamp(Tuple3&lt;Long, String, String&gt; arg0) {
        return arg0.f0;
    }

});

DataStream&lt;Tuple2&lt;Long, String&gt;&gt; input2 = ...;
input2 = input2.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;Tuple2&lt;Long, String&gt;&gt;() {
    @Override
    public long extractAscendingTimestamp(Tuple2&lt;Long, String&gt; stringStringTuple2) {
        return stringStringTuple2.f0;
    }
});

input1.coGroup(input2).where(new KeySelector&lt;Tuple3&lt;Long, String, String&gt;, String&gt;() {
    @Override
    public String getKey(Tuple3&lt;Long, String, String&gt; itemEntity) throws Exception {
        return itemEntity.f1;
    }
})
.equalTo(new KeySelector&lt;Tuple2&lt;Long, String&gt;, String&gt;() {
    @Override
    public String getKey(Tuple2&lt;Long, String&gt; value) throws Exception {
        return value.f1;
    }
})
.window(TumblingEventTimeWindows.of(Time.minutes(1)))
.apply(new CoGroupFunction&lt;Tuple3&lt;Long, String, String&gt;, Tuple2&lt;Long, String&gt;, String&gt;() {
    @Override
    public void coGroup(Iterable&lt;Tuple3&lt;Long, String, String&gt;&gt; first,
            Iterable&lt;Tuple2&lt;Long, String&gt;&gt; second, Collector&lt;String&gt; collector) throws Exception {
        StringBuilder buffer = new StringBuilder();
        buffer.append(&quot;DataStream first:\n&quot;);
        for (Tuple3&lt;Long, String, String&gt; value : first) {
            buffer.append(value).append(&quot;\n&quot;);
        }
        buffer.append(&quot;DataStream second:\n&quot;);
        for (Tuple2&lt;Long, String&gt; value : second) {
            buffer.append(value.f0).append(&quot;=&gt;&quot;).append(value.f1).append(&quot;\n&quot;);
        }
        collector.collect(buffer.toString());
    }
})
.print();
</code></pre>
<p>上面的例子，左流有三个元素 <code>Tuple3&lt;String,String,String&gt;</code>，右流有两个元素<code>Tuple2&lt;String,String&gt;</code>
两个流第一个元素相互关联。分别指定两个流的事件时间字段。
两个流关联后，按照EventTime划分窗口。与单流类似。
不管元素是否可以关联上，都会输出</p>
<p>用户可以定义CoGroupFunction函数, 可以实现在窗口内，任意组合，如笛卡尔积</p>
<h2 id="intervaljoin"><a class="header" href="#intervaljoin">intervalJoin()</a></h2>
<p>join()和coGroup()都是基于窗口做关联的。但是在某些情况下，两条流的数据步调未必一致。例如，订单流的数据有可能在点击流的购买动作发生之后很久才被写入，如果用窗口来圈定，很容易join不上。所以Flink又提供了&quot;<strong>Interval join</strong>&quot;的语义，按照指定字段以及<strong>右流相对左流偏移的时间区间</strong>进行关联，即：</p>
<blockquote>
<p>$right.timestamp ∈ [left.timestamp + lowerBound; left.timestamp + upperBound]$</p>
</blockquote>
<p><img src="vx_images/4903841188593.png" alt="" /></p>
<p>interval join也是inner join，虽然不需要开窗，但是需要用户指定偏移区间的上下界，并且<strong>只支持事件时间</strong>。</p>
<p>示例代码如下。注意在运行之前，需要分别在两个流上应用assignTimestampsAndWatermarks()方法获取事件时间戳和水印。</p>
<pre><code class="language-java">clickRecordStream
  .keyBy(record -&gt; record.getMerchandiseId())
  .intervalJoin(orderRecordStream.keyBy(record -&gt; record.getMerchandiseId()))
  .between(Time.seconds(-30), Time.seconds(30))
  .process(new ProcessJoinFunction&lt;AnalyticsAccessLogRecord, OrderDoneLogRecord, String&gt;() {
    @Override
    public void processElement(AnalyticsAccessLogRecord accessRecord, OrderDoneLogRecord orderRecord, Context context, Collector&lt;String&gt; collector) throws Exception {
      collector.collect(StringUtils.join(Arrays.asList(
        accessRecord.getMerchandiseId(),
        orderRecord.getPrice(),
        orderRecord.getCouponMoney(),
        orderRecord.getRebateAmount()
      ), '\t'));
    }
  })
  .print().setParallelism(1);
</code></pre>
<p>由上可见，interval join与window join不同，是两个KeyedStream之上的操作，并且需要调用between()方法指定偏移区间的上下界。如果想令上下界是开区间，可以调用upperBoundExclusive()/lowerBoundExclusive()方法。</p>
<h3 id="interval-join的实现原理"><a class="header" href="#interval-join的实现原理">interval join的实现原理</a></h3>
<p>以下是KeyedStream.process(ProcessJoinFunction)方法调用的重载方法的逻辑。</p>
<pre><code class="language-java">public &lt;OUT&gt; SingleOutputStreamOperator&lt;OUT&gt; process(
        ProcessJoinFunction&lt;IN1, IN2, OUT&gt; processJoinFunction,
        TypeInformation&lt;OUT&gt; outputType) {
    Preconditions.checkNotNull(processJoinFunction);
    Preconditions.checkNotNull(outputType);
    final ProcessJoinFunction&lt;IN1, IN2, OUT&gt; cleanedUdf = left.getExecutionEnvironment().clean(processJoinFunction);
    final IntervalJoinOperator&lt;KEY, IN1, IN2, OUT&gt; operator =
        new IntervalJoinOperator&lt;&gt;(
            lowerBound,
            upperBound,
            lowerBoundInclusive,
            upperBoundInclusive,
            left.getType().createSerializer(left.getExecutionConfig()),
            right.getType().createSerializer(right.getExecutionConfig()),
            cleanedUdf
        );
    return left
        .connect(right)
        .keyBy(keySelector1, keySelector2)
        .transform(&quot;Interval Join&quot;, outputType, operator);
}
</code></pre>
<p>可见是先对两条流执行connect()和keyBy()操作，然后利用IntervalJoinOperator算子进行转换。在IntervalJoinOperator中，会利用两个MapState分别缓存左流和右流的数据。</p>
<pre><code class="language-java">private transient MapState&lt;Long, List&lt;BufferEntry&lt;T1&gt;&gt;&gt; leftBuffer;
private transient MapState&lt;Long, List&lt;BufferEntry&lt;T2&gt;&gt;&gt; rightBuffer;

@Override
public void initializeState(StateInitializationContext context) throws Exception {
    super.initializeState(context);
    this.leftBuffer = context.getKeyedStateStore().getMapState(new MapStateDescriptor&lt;&gt;(
        LEFT_BUFFER,
        LongSerializer.INSTANCE,
        new ListSerializer&lt;&gt;(new BufferEntrySerializer&lt;&gt;(leftTypeSerializer))
    ));
    this.rightBuffer = context.getKeyedStateStore().getMapState(new MapStateDescriptor&lt;&gt;(
        RIGHT_BUFFER,
        LongSerializer.INSTANCE,
        new ListSerializer&lt;&gt;(new BufferEntrySerializer&lt;&gt;(rightTypeSerializer))
    ));
}
</code></pre>
<p>其中<code>Long</code>表示事件时间戳，<code>List&lt;BufferEntry&lt;T&gt;&gt;</code>表示该时刻到来的数据记录。</p>
<p>当左流和右流有数据到达时，会分别调用<code>processElement1()</code>和<code>processElement2()</code>方法，它们都调用了<code>processElement()</code>方法，代码如下。</p>
<pre><code class="language-java">@Override
public void processElement1(StreamRecord&lt;T1&gt; record) throws Exception {
    processElement(record, leftBuffer, rightBuffer, lowerBound, upperBound, true);
}

@Override
public void processElement2(StreamRecord&lt;T2&gt; record) throws Exception {
    processElement(record, rightBuffer, leftBuffer, -upperBound, -lowerBound, false);
}

@SuppressWarnings(&quot;unchecked&quot;)
private &lt;THIS, OTHER&gt; void processElement(
        final StreamRecord&lt;THIS&gt; record,
        final MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;THIS&gt;&gt;&gt; ourBuffer,
        final MapState&lt;Long, List&lt;IntervalJoinOperator.BufferEntry&lt;OTHER&gt;&gt;&gt; otherBuffer,
        final long relativeLowerBound,
        final long relativeUpperBound,
        final boolean isLeft) throws Exception {
    final THIS ourValue = record.getValue();
    final long ourTimestamp = record.getTimestamp();
    if (ourTimestamp == Long.MIN_VALUE) {
        throw new FlinkException(&quot;Long.MIN_VALUE timestamp: Elements used in &quot; +
                &quot;interval stream joins need to have timestamps meaningful timestamps.&quot;);
    }
    if (isLate(ourTimestamp)) {
        return;
    }
    addToBuffer(ourBuffer, ourValue, ourTimestamp);
    for (Map.Entry&lt;Long, List&lt;BufferEntry&lt;OTHER&gt;&gt;&gt; bucket: otherBuffer.entries()) {
        final long timestamp  = bucket.getKey();
        if (timestamp &lt; ourTimestamp + relativeLowerBound ||
                timestamp &gt; ourTimestamp + relativeUpperBound) {
            continue;
        }
        for (BufferEntry&lt;OTHER&gt; entry: bucket.getValue()) {
            if (isLeft) {
                collect((T1) ourValue, (T2) entry.element, ourTimestamp, timestamp);
            } else {
                collect((T1) entry.element, (T2) ourValue, timestamp, ourTimestamp);
            }
        }
    }
    long cleanupTime = (relativeUpperBound &gt; 0L) ? ourTimestamp + relativeUpperBound : ourTimestamp;
    if (isLeft) {
        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_LEFT, cleanupTime);
    } else {
        internalTimerService.registerEventTimeTimer(CLEANUP_NAMESPACE_RIGHT, cleanupTime);
    }
}
</code></pre>
<p>这段代码的思路是：</p>
<ol>
<li>取得当前流<code>StreamRecord</code>的时间戳，调用<code>isLate()</code>方法判断它是否是迟到数据（即时间戳小于当前水印值），如是则丢弃。</li>
<li>调用<code>addToBuffer()</code>方法，将时间戳和数据一起插入当前流对应的<code>MapState</code>。</li>
<li>遍历另外一个流的<code>MapState</code>，如果数据满足前述的时间区间条件，则调用<code>collect()</code>方法将该条数据投递给用户定义的<code>ProcessJoinFunction</code>进行处理。
<code>collect()</code>方法的代码如下，注意结果对应的时间戳是左右流时间戳里较大的那个。</li>
</ol>
<pre><code class="language-java">private void collect(T1 left, T2 right, long leftTimestamp, long rightTimestamp) throws Exception {
    final long resultTimestamp = Math.max(leftTimestamp, rightTimestamp);
    collector.setAbsoluteTimestamp(resultTimestamp);
    context.updateTimestamps(leftTimestamp, rightTimestamp, resultTimestamp);
    userFunction.processElement(left, right, context, collector);
}
</code></pre>
<ol start="4">
<li>调用<code>TimerService.registerEventTimeTimer()</code>注册时间戳为<code>timestamp + relativeUpperBound</code>的定时器，该定时器负责在水印超过区间的上界时执行状态的清理逻辑，防止数据堆积。注意左右流的定时器所属的<code>namespace</code>是不同的，具体逻辑则位于<code>onEventTime()</code>方法中。</li>
</ol>
<pre><code class="language-java">@Override
public void onEventTime(InternalTimer&lt;K, String&gt; timer) throws Exception {
    long timerTimestamp = timer.getTimestamp();
    String namespace = timer.getNamespace();
    logger.trace(&quot;onEventTime @ {}&quot;, timerTimestamp);
    switch (namespace) {
        case CLEANUP_NAMESPACE_LEFT: {
            long timestamp = (upperBound &lt;= 0L) ? timerTimestamp : timerTimestamp - upperBound;
            logger.trace(&quot;Removing from left buffer @ {}&quot;, timestamp);
            leftBuffer.remove(timestamp);
            break;
        }
        case CLEANUP_NAMESPACE_RIGHT: {
            long timestamp = (lowerBound &lt;= 0L) ? timerTimestamp + lowerBound : timerTimestamp;
            logger.trace(&quot;Removing from right buffer @ {}&quot;, timestamp);
            rightBuffer.remove(timestamp);
            break;
        }
        default:
            throw new RuntimeException(&quot;Invalid namespace &quot; + namespace);
    }
}
</code></pre>
<h2 id="broadcast-join"><a class="header" href="#broadcast-join">broadcast join</a></h2>
<h2 id="interval-join"><a class="header" href="#interval-join">interval join</a></h2>
<h1 id="flink-sql-join"><a class="header" href="#flink-sql-join">Flink SQL Join</a></h1>
<h2 id="双流join"><a class="header" href="#双流join">双流Join</a></h2>
<h3 id="ttl"><a class="header" href="#ttl">TTL</a></h3>
<p>从 Flink 1.6 版本开始，社区引入了状态 TTL（Time-To-Live）特性。在通过Flink SQL 实现流处理时，开发者可以为作业 SQL 设置TTL，实现过期状态的自动清理，从而防止作业状态无限膨胀</p>
<pre><code class="language-sql">SELECT
    t_date,
    COUNT ( DISTINCT user_id ) AS cnt_login, -- 今日登录用户数
    COUNT ( DISTINCT CASE WHEN t_date = t_debut THEN user_id END ) AS cnt_new -- 今日新用户数
FROM
  (
    -- 计算每个用户有史以来的最小登录时间
    SELECT
       t_date,
       user_id,
       MIN (t_date) OVER (
             PARTITION BY user_id
             ORDER BY proctime
             ROWS BETWEEN 1 PRECEDING AND CURRENT ROW
       ) AS t_debut
    FROM Login
) AS t
GROUP BY t_date

</code></pre>
<h3 id="query-configuration-查询配置"><a class="header" href="#query-configuration-查询配置">Query Configuration 查询配置</a></h3>
<p>https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/streaming/query_configuration.html</p>
<p>Flink Table API 和SQL接口提供参数来调整连续查询的准确性和资源消耗。参数通过 <code>QueryConfig</code> 对象指定。<code>QueryConfig</code> 可以从 <code>TableEnvironment</code> 获得。</p>
<pre><code class="language-scala">val env = StreamExecutionEnvironment.getExecutionEnvironment
val tableEnv = TableEnvironment.getTableEnvironment(env)

// 获取 query configuration
val qConfig: StreamQueryConfig = tableEnv.queryConfig

// 设置查询参数
qConfig.withIdleStateRetentionTime(Time.hours(12), Time.hours(24))

// 定义查询和 TableSink
val result: Table = ???
val sink: TableSink[Row] = ???

// TableSink 发送结果表时传递查询参数
result.writeToSink(sink, qConfig)

// 转换为 DataStream 时传递查询参数
val stream: DataStream[Row] = result.toAppendStream[Row](qConfig)
</code></pre>
<p>空闲状态保持时间（Idle State Retention Time）参数定义一个键的状态在一次更新之后保存多久后删除。</p>
<p>通过删除键的状态，连续查询会完全忘记它之前已经看过这个键。如果删除的键再次出现，则被视为具有相应键的第一个记录。对于前面的查询示例，这意味着 sessionId 的计数从0开始。</p>
<p>配置空闲状态保存时间有两个参数：</p>
<pre><code>minimum idle state retention time，定义非活动键的状态在删除前至少保持多少时间。
maximum idle state retention time，定义非活动键的状态在删除前最多保持多少时间。
</code></pre>
<p>对于前面的查询示例：</p>
<pre><code class="language-scala">val qConfig: StreamQueryConfig = ???

// 设置 idle state retention time: min = 12 hours, max = 24 hours
qConfig.withIdleStateRetentionTime(Time.hours(12), Time.hours(24))
</code></pre>
<p>清理状态需要额外的记录，对于 minTime 和 maxTime 较大差异的情况成本更低，因此 minTime 和 maxTime 直接必须至少相差5分钟。</p>
<h2 id="维表join"><a class="header" href="#维表join">维表Join</a></h2>
<h3 id="启用asyncio"><a class="header" href="#启用asyncio">启用AsyncIO</a></h3>
<h3 id="时间表join-temporal-table-join"><a class="header" href="#时间表join-temporal-table-join">时间表Join: Temporal Table Join</a></h3>
<pre><code class="language-sql">SELECT
o.amout, o.currency, r.rate, o.amount * r.rate
FROM
Orders AS o
JOIN LatestRates FOR SYSTEM_TIME AS OF o.proctime AS r
ON r.currency = o.currency
</code></pre>
<hr />
<p>title: &quot;Flink sql window&quot;
layout: post
date: 2021-04-12 19:47:00
category: bigdata
tags:</p>
<ul>
<li>Flink</li>
<li>Flink-SQL</li>
<li>window</li>
</ul>
<h2>share: false
comments: true</h2>
<p>flink窗口函数包含滚动窗口、滑动窗口、会话窗口和OVER窗口</p>
<h2 id="flink-sql-窗口的基本概念与使用"><a class="header" href="#flink-sql-窗口的基本概念与使用">Flink SQL 窗口的基本概念与使用</a></h2>
<h3 id="滚动窗口"><a class="header" href="#滚动窗口">滚动窗口</a></h3>
<p>滚动窗口（TUMBLE）将每个元素分配到一个指定大小的窗口中。通常，滚动窗口有一个固定的大小，并且不会出现重叠。例如，如果指定了一个5分钟大小的滚动窗口，无限流的数据会根据时间划分为<code>[0:00 - 0:05)</code>、<code>[0:05, 0:10)</code>、<code>[0:10, 0:15)</code>等窗口。下图展示了一个30秒的滚动窗口。<br />
<img src="_v_images/20210412125050690_701107302" alt="" />
使用标识函数选出窗口的起始时间或者结束时间，窗口的时间属性用于下级Window的聚合。</p>
<table><thead><tr><th>窗口标识函数</th><th>返回类型</th><th>描述</th></tr></thead><tbody>
<tr><td><code>TUMBLE_START(time-attr, size-interval)</code></td><td>TIMESTAMP</td><td>返回窗口的起始时间（包含边界）。例如<code>[00:10, 00:15)</code> 窗口，返回<code>00:10</code> 。</td></tr>
<tr><td><code>TUMBLE_END(time-attr, size-interval)</code></td><td>TIMESTAMP</td><td>返回窗口的结束时间（包含边界）。例如<code>[00:00, 00:15]</code>窗口，返回<code>00:15</code>。</td></tr>
<tr><td><code>TUMBLE_ROWTIME(time-attr, size-interval)</code></td><td>TIMESTAMP(rowtime-attr)</td><td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15]</code>窗口，返回<code>00:14:59.999</code> 。返回值是一个rowtime attribute，即可以基于该字段做时间属性的操作，例如，级联窗口只能用在基于Event Time的Window上</td></tr>
<tr><td><code>TUMBLE_PROCTIME(time-attr, size-interval)</code></td><td>TIMESTAMP(rowtime-attr)</td><td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15]</code>窗口，返回<code>00:14:59.999</code>。返回值是一个proctime attribute，即可以基于该字段做时间属性的操作，例如，级联窗口只能用在基于Processing Time的Window上</td></tr>
</tbody></table>
<p>TUMBLE window示例</p>
<pre><code class="language-java">import org.apache.flink.api.common.typeinfo.TypeHint;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.tuple.Tuple3;
import org.apache.flink.streaming.api.TimeCharacteristic;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;


import java.sql.Timestamp;
import java.util.Arrays;

public class TumbleWindowExample {

    public static void main(String[] args) throws Exception {

        /**
         * 1 注册环境
         */
        EnvironmentSettings mySetting = EnvironmentSettings
                .newInstance()
//                .useOldPlanner()
                .useBlinkPlanner()
                .inStreamingMode()
                .build();

        // 获取 environment
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        // 指定系统时间概念为 event time
        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,mySetting);


        // 初始数据
        DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; log = env.fromCollection(Arrays.asList(
                //时间 14:53:00
                new Tuple3&lt;&gt;(1572591180_000L,&quot;xiao_ming&quot;,300),
                //时间 14:53:09
                new Tuple3&lt;&gt;(1572591189_000L,&quot;zhang_san&quot;,303),
                //时间 14:53:12
                new Tuple3&lt;&gt;(1572591192_000L, &quot;xiao_li&quot;,204),
                //时间 14:53:21
                new Tuple3&lt;&gt;(1572591201_000L,&quot;li_si&quot;, 208)
                ));

        // 指定时间戳
        SingleOutputStreamOperator&lt;Tuple3&lt;Long, String, Integer&gt;&gt; logWithTime = log.assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;Tuple3&lt;Long, String, Integer&gt;&gt;() {

            @Override
            public long extractAscendingTimestamp(Tuple3&lt;Long, String, Integer&gt; element) {
                return element.f0;
            }
        });

        // 转换为 Table
        Table logT = tEnv.fromDataStream(logWithTime, &quot;t.rowtime, name, v&quot;);

        Table result = tEnv.sqlQuery(&quot;SELECT TUMBLE_START(t, INTERVAL '10' SECOND) AS window_start,&quot; +
                &quot;TUMBLE_END(t, INTERVAL '10' SECOND) AS window_end, SUM(v) FROM &quot;
                + logT + &quot; GROUP BY TUMBLE(t, INTERVAL '10' SECOND)&quot;);

        TypeInformation&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt; tpinf = new TypeHint&lt;Tuple3&lt;Timestamp,Timestamp,Integer&gt;&gt;(){}.getTypeInfo();
        tEnv.toAppendStream(result, tpinf).print();

        env.execute();
    }


}

</code></pre>
<p>sql逻辑，每十秒钟聚合<br />
执行结果：</p>
<pre><code class="language-shell">(2019-11-01 06:53:00.0,2019-11-01 06:53:10.0,603)  
(2019-11-01 06:53:20.0,2019-11-01 06:53:30.0,208)  
(2019-11-01 06:53:10.0,2019-11-01 06:53:20.0,204)
</code></pre>
<h3 id="滑动窗口"><a class="header" href="#滑动窗口">滑动窗口</a></h3>
<p>滑动窗口（HOP），也被称作Sliding Window。不同于滚动窗口，滑动窗口的窗口可以重叠。</p>
<p>滑动窗口有两个参数：slide和size。slide为每次滑动的步长，size为窗口的大小。</p>
<ul>
<li>slide &lt; size，则窗口会重叠，每个元素会被分配到多个窗口。</li>
<li>slide = size，则等同于滚动窗口（TUMBLE）。</li>
<li>slide &gt; size，则为跳跃窗口，窗口之间不重叠且有间隙。</li>
</ul>
<p>通常，大部分元素符合多个窗口情景，窗口是重叠的。因此，滑动窗口在计算移动平均数（moving averages）时很实用。例如，计算过去5分钟数据的平均值，每10秒钟更新一次，可以设置slide为10秒，size为5分钟。下图为您展示间隔为30秒，窗口大小为1分钟的滑动窗口。</p>
<p><img src="_v_images/20210412125050182_1936746176" alt="滑动窗口" title="滑动窗口" /></p>
<p>使用滑动窗口标识函数选出窗口的起始时间或者结束时间，窗口的时间属性用于下级Window的聚合。</p>
<table><thead><tr><th>窗口标识函数</th><th>返回类型</th><th>描述</th></tr></thead><tbody>
<tr><td><code>HOP_START（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td><td>TIMESTAMP</td><td>返回窗口的起始时间（包含边界）。例如<code>[00:10, 00:15)</code> 窗口，返回<code>00:10</code> 。</td></tr>
<tr><td><code>HOP_END（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td><td>TIMESTAMP</td><td>返回窗口的结束时间（包含边界）。例如<code>[00:00, 00:15)</code> 窗口，返回<code>00:15</code>。</td></tr>
<tr><td><code>HOP_ROWTIME（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td><td>TIMESTAMP（rowtime-attr）</td><td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15)</code> 窗口，返回<code>00:14:59.999</code>。返回值是一个rowtime attribute，即可以基于该字段做时间类型的操作，只能用在基于event time的window上。</td></tr>
<tr><td><code>HOP_PROCTIME（&lt;time-attr&gt;, &lt;slide-interval&gt;, &lt;size-interval&gt;）</code></td><td>TIMESTAMP（rowtime-attr）</td><td>返回窗口的结束时间（不包含边界）。例如<code>[00:00, 00:15)</code> 窗口，返回<code>00:14:59.999</code> 。返回值是一个proctime attribute</td></tr>
</tbody></table>
<p>滑动窗口实例：<br />
java代码同上，sql语句改为：</p>
<pre><code class="language-sql">SELECT HOP_START(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) AS window_start, HOP_END(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) AS window_end, SUM(v) FROM   logT   GROUP BY HOP(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND)
</code></pre>
<p>每间隔5秒统计10秒内的数据<br />
sql结果如下：</p>
<pre><code class="language-shell">(2019-11-01 06:53:15.0,2019-11-01 06:53:25.0,208)  
(2019-11-01 06:53:10.0,2019-11-01 06:53:20.0,204)  
(2019-11-01 06:53:05.0,2019-11-01 06:53:15.0,507)  
(2019-11-01 06:53:20.0,2019-11-01 06:53:30.0,208)  
(2019-11-01 06:53:00.0,2019-11-01 06:53:10.0,603)  
(2019-11-01 06:52:55.0,2019-11-01 06:53:05.0,300)
</code></pre>
<h3 id="会话窗口"><a class="header" href="#会话窗口">会话窗口</a></h3>
<p>会话窗口（SESSION）通过Session活动来对元素进行分组。会话窗口与滚动窗口和滑动窗口相比，没有窗口重叠，没有固定窗口大小。相反，当它在一个固定的时间周期内不再收到元素，即会话断开时，这个窗口就会关闭。</p>
<p>会话窗口通过一个间隔时间（Gap）来配置，这个间隔定义了非活跃周期的长度。例如，一个表示鼠标点击活动的数据流可能具有长时间的空闲时间，并在两段空闲之间散布着高浓度的点击。 如果数据在指定的间隔（Gap）之后到达，则会开始一个新的窗口。</p>
<p>会话窗口示例如下图。每个Key由于不同的数据分布，形成了不同的Window。</p>
<p><img src="_v_images/20210412125049773_363551252" alt="" /><br />
使用标识函数选出窗口的起始时间或者结束时间，窗口的时间属性用于下级Window的聚合。</p>
<table><thead><tr><th>窗口标识函数</th><th>返回类型</th><th>描述</th></tr></thead><tbody>
<tr><td><code>SESSION_START（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td><td>Timestamp</td><td>返回窗口的起始时间（包含边界）。如<code>[00:10, 00:15)</code> 的窗口，返回 <code>00:10</code> ，即为此会话窗口内第一条记录的时间。</td></tr>
<tr><td><code>SESSION_END（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td><td>Timestamp</td><td>返回窗口的结束时间（包含边界）。如<code>[00:00, 00:15)</code> 的窗口，返回 <code>00:15</code>，即为此会话窗口内最后一条记录的时间+<code>&lt;gap-interval&gt;</code>。</td></tr>
<tr><td><code>SESSION_ROWTIME（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td><td>Timestamp（rowtime-attr）</td><td>返回窗口的结束时间（不包含边界）。如 <code>[00:00, 00:15)</code> 的窗口，返回<code>00:14:59.999</code> 。返回值是一个rowtime attribute，也就是可以基于该字段进行时间类型的操作。该参数只能用于基于event time的window 。</td></tr>
<tr><td><code>SESSION_PROCTIME（&lt;time-attr&gt;, &lt;gap-interval&gt;）</code></td><td>Timestamp（rowtime-attr）</td><td>返回窗口的结束时间（不包含边界）。如 <code>[00:00, 00:15)</code> 的窗口，返回 <code>00:14:59.999</code> 。返回值是一个 proctime attribute，也就是可以基于该字段进行时间类型的操作。该参数只能用于基于processing time的window 。</td></tr>
</tbody></table>
<p>会话窗口实例：<br />
java代码同上<br />
sql语句如下：<br />
每隔5秒聚合</p>
<pre><code class="language-sql">SELECT SESSION_START(t, INTERVAL '5' SECOND) AS window_start,
SESSION_END(t, INTERVAL '5' SECOND) AS window_end, SUM(v) FROM  logT  GROUP BY SESSION(t, INTERVAL '5' SECOND)
</code></pre>
<p>sql结果：</p>
<pre><code class="language-shell">(2019-11-01 06:53:21.0,2019-11-01 06:53:26.0,208)  
(2019-11-01 06:53:00.0,2019-11-01 06:53:05.0,300)  
(2019-11-01 06:53:09.0,2019-11-01 06:53:17.0,507)

</code></pre>
<h3 id="over窗口"><a class="header" href="#over窗口">OVER窗口</a></h3>
<p>OVER窗口（OVER Window）是传统数据库的标准开窗，不同于Group By Window，OVER窗口中每1个元素都对应1个窗口。窗口内的元素是当前元素往前多少个或往前多长时间的元素集合，因此流数据元素分布在多个窗口中。</p>
<p>在应用OVER窗口的流式数据中，每1个元素都对应1个OVER窗口。每1个元素都触发1次数据计算，每个触发计算的元素所确定的行，都是该元素所在窗口的最后1行。在实时计算的底层实现中，OVER窗口的数据进行全局统一管理（数据只存储1份），逻辑上为每1个元素维护1个OVER窗口，为每1个元素进行窗口计算，完成计算后会清除过期的数据。</p>
<p>Flink SQL中对OVER窗口的定义遵循标准SQL的定义语法，传统OVER窗口没有对其进行更细粒度的窗口类型命名划分。按照计算行的定义方式，OVER Window可以分为以下两类：</p>
<ul>
<li>ROWS OVER Window：每一行元素都被视为新的计算行，即每一行都是一个新的窗口。</li>
<li>RANGE OVER Window：具有相同时间值的所有元素行视为同一计算行，即具有相同时间值的所有行都是同一个窗口。</li>
</ul>
<p>Rows OVER Window语义</p>
<p>窗口数据</p>
<p>ROWS OVER Window的每个元素都确定一个窗口。ROWS OVER Window分为Unbounded（无界流）和Bounded（有界流）两种情况。<br />
Unbounded ROWS OVER Window数据示例如下图所示。<br />
<img src="_v_images/20210412125049063_1513990833" alt="" /></p>
<p>虽然上图所示窗口user1的w7、w8及user2的窗口w3、w4都是同一时刻到达，但它们仍然在不同的窗口，这一点与RANGE OVER Window不同。</p>
<p>Bounded ROWS OVER Window数据以3个元素（往前2个元素）的窗口为例，如下图所示。</p>
<p><img src="_v_images/20210412125048555_824932679" alt="" /></p>
<p>虽然上图所示窗口user1的w5、w6及user2的窗口w1、w2都是同一时刻到达，但它们仍然在不同的窗口，这一点与RANGE OVER Window不同。</p>
<p>RANGE OVER Window语义</p>
<p>窗口数据</p>
<p>RANGE OVER Window所有具有共同元素值（元素时间戳）的元素行确定一个窗口，RANGE OVER Window分为Unbounded和Bounded的两种情况。<br />
Unbounded RANGE OVER Window数据示例如下图所示。</p>
<p><img src="_v_images/20210412125048048_1968314666" alt="" /><br />
上图所示窗口user1的w7、user2的窗口w3，两个元素同一时刻到达，属于相同的window，这一点与ROWS OVER Window不同。</p>
<p>Bounded RANGE OVER Window数据，以3秒中数据<code>(INTERVAL '2' SECOND)</code>的窗口为例，如下图所示。</p>
<p><img src="_v_images/20210412125047258_2027952546" alt="" /></p>
<p>上图所示窗口user1的w6、user2的窗口w3，元素都是同一时刻到达，属于相同的window，这一点与ROWS OVER Window不同。</p>
<p>OVER窗口实例：<br />
java代码同上<br />
初始数据如下：</p>
<pre><code class="language-java">// 初始数据
DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; log = env.fromCollection(Arrays.asList(
        //时间 14:53:00
        new Tuple3&lt;&gt;(1572591180_000L,&quot;xiao_ming&quot;,999),
        //时间 14:53:09
        new Tuple3&lt;&gt;(1572591189_000L,&quot;zhang_san&quot;,303),
        //时间 14:53:12
        new Tuple3&lt;&gt;(1572591192_000L, &quot;xiao_li&quot;,888),
        //时间 14:53:21
        new Tuple3&lt;&gt;(1572591201_000L,&quot;li_si&quot;, 908),
        //2019-11-01 14:53:31
        new Tuple3&lt;&gt;(1572591211_000L,&quot;li_si&quot;, 555),
        //2019-11-01 14:53:41
        new Tuple3&lt;&gt;(1572591221_000L,&quot;zhang_san&quot;, 666),
        //2019-11-01 14:53:51
        new Tuple3&lt;&gt;(1572591231_000L,&quot;xiao_ming&quot;, 777),
        //2019-11-01 14:54:01
        new Tuple3&lt;&gt;(1572591241_000L,&quot;xiao_ming&quot;, 213),
        //2019-11-01 14:54:11
        new Tuple3&lt;&gt;(1572591251_000L,&quot;zhang_san&quot;, 300),
        //2019-11-01 14:54:21
        new Tuple3&lt;&gt;(1572591261_000L,&quot;li_si&quot;, 112)
));
</code></pre>
<p>ROWS over Windown sql语句如下：</p>
<pre><code class="language-sql">SELECT name,v,MAX(v) OVER(
PARTITION BY name 
ORDER BY t 
ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
) FROM logT
</code></pre>
<p>sql结果如下：</p>
<pre><code class="language-shell">(zhang_san,303,303)  
(xiao_li,888,888)  
(li_si,908,908)  
(xiao_ming,999,999)  
(zhang_san,666,666)  
(li_si,555,908)  
(xiao_ming,777,999)  
(li_si,112,908)  
(zhang_san,300,666)  
(xiao_ming,213,999)

</code></pre>
<p>RANGE OVER Window sql 语句如下：</p>
<pre><code class="language-sql">SELECT name,v,MAX(v) OVER(
PARTITION BY name 
ORDER BY t 
RANGE BETWEEN INTERVAL '15' SECOND PRECEDING AND CURRENT ROW
) FROM  logT
</code></pre>
<p>sql结果如下：</p>
<pre><code class="language-shell">(xiao_ming,999,999)  
(xiao_li,888,888)  
(zhang_san,303,303)  
(li_si,908,908)  
(li_si,555,908)  
(xiao_ming,777,777)  
(zhang_san,666,666)  
(li_si,112,112)  
(xiao_ming,213,777)  
(zhang_san,300,300)
</code></pre>
<p>本文的java代码来自：
<a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/table/src/main/java/sql/window">https://github.com/CheckChe08...</a></p>
<h2 id="底层实现"><a class="header" href="#底层实现">底层实现</a></h2>
<p><a href="https://mp.weixin.qq.com/s/UkpkS_JiRGR0ibZKYechbg">https://mp.weixin.qq.com/s/UkpkS_JiRGR0ibZKYechbg</a></p>
<h3 id="概述-1"><a class="header" href="#概述-1">概述</a></h3>
<p>窗口是无限流上一种核心机制，可以流分割为有限大小的“窗口”，同时，在窗口内进行聚合，从而把源源不断产生的数据根据不同的条件划分成一段一段有边界的数据区间，使用户能够利用窗口功能实现很多复杂的统计分析需求。</p>
<h3 id="window分类"><a class="header" href="#window分类">Window分类</a></h3>
<p>1、TimeWindow与CountWindow Flink Window可以是时间驱动的（<code>TimeWindow</code>），也可以是数据驱动的（CountWindow）。由于flink-planner-blink SQL中目前只支持TimeWindow相应的表达语句（<code>TUMBLE</code>、<code>HOP</code>、<code>SESSION</code>），因此，本文主要介绍TimeWindow SQL示例和逻辑，CountWindow感兴趣的读者可自行分析。</p>
<p>2、TimeWindow子类型 Flink TimeWindow有滑动窗口(<code>HOP</code>)、滚动窗口(<code>TUMBLE</code>)以及会话窗口(<code>SESSION</code>)三种，所选取的字段时间，可以是系统时间(<code>PROCTIME</code>)或事件时间(<code>EVENT TIME</code>)两种，接来下依次介绍。</p>
<h4 id="tumble-window滚动窗口"><a class="header" href="#tumble-window滚动窗口">Tumble Window（滚动窗口）</a></h4>
<p>翻转窗口Assigner将每个元素分配给具有指定大小的窗口。翻转窗口的大小是固定的，且不会重叠。例如，指定一个大小为5分钟的翻滚窗口，并每5分钟启动一个新窗口，如下图所示：</p>
<p><img src="_v_images/20210412125430504_1844331455" alt="图片" /></p>
<p>TUMBLE ROWTIME语法示例：</p>
<pre><code class="language-sql">CREATE TABLE sessionOrderTableRowtime (
    ctime TIMESTAMP,
    categoryName VARCHAR,
    shopName VARCHAR,
    itemName VARCHAR,
    userId VARCHAR,
    price FLOAT,
    action BIGINT,
    WATERMARK FOR ctime AS withOffset(ctime, 1000),
    proc AS PROCTIME()
) with (
    `type` = 'kafka',
    format = 'json',
    updateMode = 'append',
    `group.id` = 'groupId',
    bootstrap.servers = 'xxxxx:9092',
    version = '0.10',
    `zookeeper.connect` = 'xxxxx:2181',
    startingOffsets = 'latest',
    topic = 'sessionsourceproctime'
);


CREATE TABLE popwindowsink (
    countA BIGINT,
    ctime_start TIMESTAMP,
    ctime_end VARCHAR,
    ctime_rowtime VARCHAR,
    categoryName VARCHAR,
    price_sum FLOAT
) with (
    format = 'json',
    updateMode = 'append',
    bootstrap.servers = 'xxxxx:9092',
    version = '0.10',
    topic = 'sessionsinkproctime',
    `type` = 'kafka'
);

INSERT INTO popwindowsink
(SELECT
COUNT(*),
TUMBLE_START(ctime, INTERVAL '5' MINUTE),
DATE_FORMAT(TUMBLE_END(ctime, INTERVAL '5' MINUTE), 'yyyy-MM-dd-HH-mm-ss:SSS'), --将TUMBLE_END转为可视化的日期
DATE_FORMAT(TUMBLE_ROWTIME(ctime, INTERVAL '5' MINUTE), 'yyyy-MM-dd-HH-mm-ss:SSS'), --这里TUMBLE_ROWTIME为TUMBLE_END-1ms，一般用于后续窗口级联聚合
categoryName,
SUM(price)
FROM sessionOrderTableRowtime
GROUP BY TUMBLE(ctime, INTERVAL '5' MINUTE), categoryName)
</code></pre>
<p>TUMBLEP ROCTIME语法示例：</p>
<pre><code class="language-sql">INSERT INTO popwindowsink
(SELECT
COUNT(*),
TUMBLE_START(proc, INTERVAL '5' MINUTE),
DATE_FORMAT(TUMBLE_END(proc, INTERVAL '5' MINUTE), 'yyyy-MM-dd-HH-mm-ss:SSS'),
DATE_FORMAT(TUMBLE_PROCTIME(proc, INTERVAL '5' MINUTE), 'yyyy-MM-dd-HH-mm-ss:SSS'), --注意这里proc字段即Source DDL中指定的PROCTIME
categoryName,
SUM(price)
FROM sessionOrderTableRowtime
GROUP BY TUMBLE(proc, INTERVAL '5' MINUTE), categoryName)
</code></pre>
<p>ROWTIME与PROCTIME区别：</p>
<ul>
<li>在使用上：主要是填入的ctime、proc关键字的区别，这两个字段在Source DDL中指定方式不一样.</li>
<li>在实现原理上：ROWTIME模式，根据ctime对应的值，去确定窗口的start、end；PROCTIME模式，在WindowOperator处理数据时，获取本地系统时间，去确定窗口的start、end.</li>
</ul>
<p>由于生产系统中，主要使用ROWTIME来计算、聚合、统计，PROCTIME一般用于测试或对统计精度要求不高的场景，本文后续都主要以ROWTIME进行分析。</p>
<h4 id="hop-window滑动窗口"><a class="header" href="#hop-window滑动窗口">Hop Window（滑动窗口）</a></h4>
<p>滑动窗口Assigner将元素分配给多个固定长度的窗口。类似于滚动窗口分配程序，窗口的大小由窗口大小参数配置。因此，如果滑动窗口小于窗口大小，则滑动窗口可以重叠。在这种情况下，元素被分配到多个窗口。其实，滚动窗口TUMBLE是滑动窗口的一个特例。例子，设置一个10分钟长度的窗口，以5分钟间隔滑动。这样，每5分钟就会出现一个窗口，其中包含最近10分钟内到达的事件，如下图：</p>
<p><img src="_v_images/20210412125430097_940076266" alt="图片" /></p>
<p>HOP ROWTIME语法示例：</p>
<pre><code class="language-sql">INSERT INTO popwindowsink
(SELECT
COUNT(*),
HOP_START(ctime, INTERVAL '5' MINUTE,  INTERVAL '10' MINUTE),
DATE_FORMAT(HOP_END(ctime, INTERVAL '5' MINUTE,  INTERVAL '10' MINUTE), 'yyyy-MM-dd-HH-mm-ss:SSS'),
DATE_FORMAT(HOP_ROWTIME(ctime, INTERVAL '5' MINUTE,  INTERVAL '10' MINUTE), 'yyyy-MM-dd-HH-mm-ss:SSS'), --注意这里ctime字段即Source DDL中指定的ROWTIME
categoryName,
SUM(price)
FROM sessionOrderTableRowtime
GROUP BY HOP(ctime, INTERVAL '5' MINUTE,  INTERVAL '10' MINUTE), categoryName)
</code></pre>
<h4 id="session-window会话窗口"><a class="header" href="#session-window会话窗口">Session Window（会话窗口）</a></h4>
<p>会话窗口Assigner根据活动会话对元素进行分组。与翻滚窗口和滑动窗口相比，会话窗口不会重叠，也没有固定的开始和结束时间。相反，会话窗口在一段时间内不接收元素时关闭，即，当一段不活跃的间隙发生时，当前会话关闭，随后的元素被分配给新的会话。</p>
<p><img src="_v_images/20210412125429689_1075804874" alt="图片" /></p>
<p>SESSION ROWTIME语法示例：</p>
<pre><code class="language-sql">INSERT INTO popwindowsink
(SELECT
COUNT(*),
SESSION_START(ctime, INTERVAL '5' MINUTE),
DATE_FORMAT(SESSION_END(ctime, INTERVAL '5' MINUTE, 'yyyy-MM-dd-HH-mm-ss:SSS'),
DATE_FORMAT(SESSION_ROWTIME(ctime, INTERVAL '5' MINUTE), 'yyyy-MM-dd-HH-mm-ss:SSS'), --注意这里ctime字段即Source DDL中指定的ROWTIME
categoryName,
SUM(price)
FROM sessionOrderTableRowtime
GROUP BY SESSION(ctime, INTERVAL '5' MINUTE), categoryName)
</code></pre>
<h3 id="window分类及整体流程"><a class="header" href="#window分类及整体流程">Window分类及整体流程</a></h3>
<p><img src="_v_images/20210412125429282_375710651" alt="图片" /></p>
<p>上图内部流程分析：</p>
<p>应用层SQL:
1.1 window分类及配置，包括滑动、翻转、会话类型窗口
1.2 window时间类型配置，默认待字段名的EventTime，也可以通过PROCTIME()配置为ProcessingTime
Calcite解析引擎:
2.1 Calcite SQL解析，包括逻辑、优化、物理计划和算子绑定(#translateToPlanInternal)，在本文特指StreamExecGroupWindowAggregateRule和StreamExecGroupWindowAggregate物理计划
WindowOperator算子创建相关:
3.1 StreamExecGroupWindowAggregate#createWindowOperator创建算子
3.2 WindowAssigner的创建，根据输入的数据，和窗口类型，生成多个窗口
3.3 processElement()真实处理数据，包括聚合运算，生成窗口，更新缓存，提交数据等功能
3.4 Trigger根据数据或时间，来决定窗口触发</p>
<h3 id="创建windowoperator算子"><a class="header" href="#创建windowoperator算子">创建WindowOperator算子</a></h3>
<p>由于window语法主要是在group by语句中使用，calcite创建WindowOperator算子伴随着聚合策略的实现，包括聚合规则匹配(StreamExecGroupWindowAggregateRule)，以及生成聚合physical算子StreamExecGroupWindowAggregate两个子流程：</p>
<p><img src="_v_images/20210412125428973_1212879820" alt="图片" /></p>
<p>上图内部流程分析：</p>
<p>a. StreamExecGroupWindowAggregateRule会对window进行提前匹配，
生成的WindowEmitStrategy内部具有：是否为EventTime表标识、是否为SessionWindow、early fire和late fire配置、延迟毫秒数（窗口结束时间加上这个毫秒数即数据清理时间）
b. StreamExecGroupWindowAggregateRule会获取聚合逻辑计划中，window配置的时间字段，记录时间字段index信息，window的触发和清理都会用到这个时间
c. StreamExecGroupWindowAggregate入口即为translateToPlanInternal，它的实现方式与spark比较类似，会先循环调用child子节点translateToPlan方法，生成inputtranform信息作为输入
d.创建aggregateHandler是一个代码生成的过程，其生成的创建的class实现了accumulate、retract、merge、update方法，这个handler最后也传递给了WindowOperater，处理数据时，可以进行聚合、回撤并输出最新数据给下游
e. StreamExecGroupWindowAggregate与window相关的最后一步就是调用#createWindowOperator创建算子，其内部先创建了一个WindowOperatorBuilder，设置window类型、retract标识、trigger(window触发条件)、聚合函数句柄等，最后创建WindowOperator</p>
<h3 id="windowoperator处理数据图解"><a class="header" href="#windowoperator处理数据图解">WindowOperator处理数据图解</a></h3>
<p>在上一小节，已经完成了WindowOperator参数的设定，并创建实例，接下来我们主要分析WindowOperator真实处理数据的流程(起点在WindowOperator#processElement方法)：</p>
<p><img src="_v_images/20210412125428666_1517359800" alt="图片" /></p>
<p>processElement处理数据流程：</p>
<p>a、 获取当前record具有的事件时间，如果是Processing Time模式，从时间服务Service里面获取时间即可
b、使用上一步获取的时间，接着调用windowFunction.assignWindow生成窗口，其内部实际上是调用各类型的WindowAssigner生成窗口，windowFunction有三大类，分别是Paned（滑动）、Merge（会话）、General（前两种以外的），WindowAssigner类型大致有5类，分别是Tumbling（翻转）、Sliding（滑动）、Session（会话）、CountTumbling 、CountSlide这几类,根据输入的一条数据和时间，可以生成1到多个窗口
c、接下来是遍历涉及的窗口进行聚合，包括从windowState获取聚合前值、使用句柄进行聚合、更新状态至windowState，将当前转态
d、上一步聚合完成后，就可以遍历窗口，使用TriggerContext（其实就是不同类型窗口Trigger触发器的代理），综合early fire、late fire、水印时间与窗口结束时间，综合判断是否触发窗口写出
e、如果TriggerContext判断出触发条件为true，则调用emitWindowResult写出，其内部有retract判断，更新当前state及previous state，写出数据等操作
f、如果TriggerContext判断出触发条件为false，则触发需要注册cleanupTimer,到达指定时间后，触发onEventTime或onProcessingTime
g、onEventTime或onProcessingTime功能十分类似，首先会触发emitWindowResult提交结果，另外会判断窗口结束时间+Lateness和当前时间是否相等，相等则表示可以清除窗口数据、当前state及previous state、窗口对应trigger。</p>
<h3 id="windowoperator源码调试"><a class="header" href="#windowoperator源码调试">WindowOperator源码调试</a></h3>
<p>为了更直观的理解Window内部运行原理，这里我们引入一个Flink源码中已有的SQL Window测试用例，并进行了简单的修改（即修改为使用HOP滑动窗口）</p>
<pre><code class="language-java">classWindowJoinITCase{
  @Test
  def testRowTimeInnerJoinWithWindowAggregateOnFirstTime(): Unit = {
    val sqlQuery =
      &quot;&quot;&quot;
        |SELECT t1.key, HOP_END(t1.rowtime, INTERVAL '4' SECOND, INTERVAL '20' SECOND), COUNT(t1.key)
        |FROM T1 AS t1
        |GROUP BY HOP(t1.rowtime, INTERVAL '4' SECOND, INTERVAL '20' SECOND), t1.key
        |&quot;&quot;&quot;.stripMargin

    val data1 = new mutable.MutableList[(String, String, Long)]
    data1.+=((&quot;A&quot;, &quot;L-1&quot;, 1000L))
    data1.+=((&quot;A&quot;, &quot;L-2&quot;, 2000L))
    data1.+=((&quot;A&quot;, &quot;L-3&quot;, 3000L))
    //data1.+=((&quot;B&quot;, &quot;L-8&quot;, 2000L))
    data1.+=((&quot;B&quot;, &quot;L-4&quot;, 4000L))
    data1.+=((&quot;C&quot;, &quot;L-5&quot;, 2100L))
    data1.+=((&quot;A&quot;, &quot;L-6&quot;, 10000L))
    data1.+=((&quot;A&quot;, &quot;L-7&quot;, 13000L))

    val t1 = env.fromCollection(data1)
      .assignTimestampsAndWatermarks(new Row3WatermarkExtractor2)
      .toTable(tEnv, 'key, 'id, 'rowtime)

    tEnv.registerTable(&quot;T1&quot;, t1)

    val sink = new TestingAppendSink
    val t_r = tEnv.sqlQuery(sqlQuery)
    val result = t_r.toAppendStream[Row]
    result.addSink(sink)
    env.execute()
  }
}
</code></pre>
<p>1、StreamExecGroupWindowAggregate#createWindowOperator()创建算子</p>
<p>StreamExecGroupWindowAggregate#createWindowOperator()是创建WindowOperator算子的地方，对应的代码和注释：</p>
<pre><code class="language-java">class StreamExecGroupWindowAggregate{
  private def createWindowOperator(
      config: TableConfig,
      aggsHandler: GeneratedNamespaceAggsHandleFunction[_],
      recordEqualiser: GeneratedRecordEqualiser,
      accTypes: Array[LogicalType],
      windowPropertyTypes: Array[LogicalType],
      aggValueTypes: Array[LogicalType],
      inputFields: Seq[LogicalType],
      timeIdx: Int): WindowOperator[_, _] = {

    val builder = WindowOperatorBuilder
      .builder()
      .withInputFields(inputFields.toArray)
    val timeZoneOffset = -config.getTimeZone.getOffset(Calendar.ZONE_OFFSET)

    // 设置WindowOperatorBuilder，最后通过Builder创建WindowOperator
    val newBuilder = window match {
      case TumblingGroupWindow(_, timeField, size) //Tumble PROCTIME模式，内部设置Assiger
          if isProctimeAttribute(timeField) &amp;&amp; hasTimeIntervalType(size) =&gt;
        builder.tumble(toDuration(size), timeZoneOffset).withProcessingTime()

      case TumblingGroupWindow(_, timeField, size) //Tumble ROWTIME模式，内部设置Assiger
          if isRowtimeAttribute(timeField) &amp;&amp; hasTimeIntervalType(size) =&gt;
        builder.tumble(toDuration(size), timeZoneOffset).withEventTime(timeIdx)

      case SlidingGroupWindow(_, timeField, size, slide) //HOP PROCTIME模式，内部设置Assiger
          if isProctimeAttribute(timeField) &amp;&amp; hasTimeIntervalType(size) =&gt;
        builder.sliding(toDuration(size), toDuration(slide), timeZoneOffset)
          .withProcessingTime()
       .....
      case SessionGroupWindow(_, timeField, gap)
          if isRowtimeAttribute(timeField) =&gt;
        builder.session(toDuration(gap)).withEventTime(timeIdx)
    }

    // Retraction和Trigger设置
    //默认是no retract和EventTime.afterEndOfWindow
    if (emitStrategy.produceUpdates) {
      // mark this operator will send retraction and set new trigger
      newBuilder
        .withSendRetraction()
        .triggering(emitStrategy.getTrigger)
    }

    newBuilder
      .aggregate(aggsHandler, recordEqualiser, accTypes, aggValueTypes, windowPropertyTypes)
      .withAllowedLateness(Duration.ofMillis(emitStrategy.getAllowLateness))
      .build()
  }
}
</code></pre>
<p>2、WindowOperator#processElement()处理数据，注册Timer</p>
<pre><code class="language-java">class StreamExecGroupWindowAggregate{
  private def createWindowOperator(
      config: TableConfig,
      aggsHandler: GeneratedNamespaceAggsHandleFunction[_],
      recordEqualiser: GeneratedRecordEqualiser,
      accTypes: Array[LogicalType],
      windowPropertyTypes: Array[LogicalType],
      aggValueTypes: Array[LogicalType],
      inputFields: Seq[LogicalType],
      timeIdx: Int): WindowOperator[_, _] = {

    val builder = WindowOperatorBuilder
      .builder()
      .withInputFields(inputFields.toArray)
    val timeZoneOffset = -config.getTimeZone.getOffset(Calendar.ZONE_OFFSET)

    // 设置WindowOperatorBuilder，最后通过Builder创建WindowOperator
    val newBuilder = window match {
      case TumblingGroupWindow(_, timeField, size) //Tumble PROCTIME模式，内部设置Assiger
          if isProctimeAttribute(timeField) &amp;&amp; hasTimeIntervalType(size) =&gt;
        builder.tumble(toDuration(size), timeZoneOffset).withProcessingTime()

      case TumblingGroupWindow(_, timeField, size) //Tumble ROWTIME模式，内部设置Assiger
          if isRowtimeAttribute(timeField) &amp;&amp; hasTimeIntervalType(size) =&gt;
        builder.tumble(toDuration(size), timeZoneOffset).withEventTime(timeIdx)

      case SlidingGroupWindow(_, timeField, size, slide) //HOP PROCTIME模式，内部设置Assiger
          if isProctimeAttribute(timeField) &amp;&amp; hasTimeIntervalType(size) =&gt;
        builder.sliding(toDuration(size), toDuration(slide), timeZoneOffset)
          .withProcessingTime()
       .....
      case SessionGroupWindow(_, timeField, gap)
          if isRowtimeAttribute(timeField) =&gt;
        builder.session(toDuration(gap)).withEventTime(timeIdx)
    }

    // Retraction和Trigger设置
    //默认是no retract和EventTime.afterEndOfWindow
    if (emitStrategy.produceUpdates) {
      // mark this operator will send retraction and set new trigger
      newBuilder
        .withSendRetraction()
        .triggering(emitStrategy.getTrigger)
    }

    newBuilder
      .aggregate(aggsHandler, recordEqualiser, accTypes, aggValueTypes, windowPropertyTypes)
      .withAllowedLateness(Duration.ofMillis(emitStrategy.getAllowLateness))
      .build()
  }
}
</code></pre>
<p>运行数据：</p>
<p><img src="_v_images/20210412125428458_505070513" alt="图片" /></p>
<p>3、Timer触发 I、InternalTimerServiceImpl#advanceWatermark()</p>
<p>WindowOperator#onEventTime()的调用前，可以先看其上层调用：InternalTimerServiceImpl#advanceWatermark()</p>
<p><img src="_v_images/20210412125428150_761077256" alt="图片" /></p>
<p>当获取的watermark为9999L时，把eventTimeTimerQueue队列中所有小于这个值的timer poll出来，调用WindowOperator.onEnventTime(timer)</p>
<p>II、WindwOperator#onEventTime()</p>
<p>WindwOperator#onEventTime()方法比较清晰，主要是window的触发和window的清理两段逻辑：</p>
<pre><code class="language-java">public class WindowOperator{
    publicvoidonEventTime(InternalTimer&lt;K, W&gt; timer) throws Exception {
        setCurrentKey(timer.getKey());

        triggerContext.window = timer.getNamespace();
        if (triggerContext.onEventTime(timer.getTimestamp())) {
            // fire
            emitWindowResult(triggerContext.window);
        }

        if (windowAssigner.isEventTime()) {
            windowFunction.cleanWindowIfNeeded(triggerContext.window, timer.getTimestamp());
        }
    }
}
</code></pre>
<p>III、emitWindowResult()提交结果</p>
<p>#emitWindowResult()重点关注下其第一行代码：BaseRow aggResult = windowFunction.getWindowAggregationResult(window); 这个表示根据具体的TimeWindow{start=4000, end=24000}，去获取聚合数据，如果是滑动窗口，需要将4000, 8000 ,12000，16000 , 20000, 24000这几段affect窗口里面的聚合值合并起来，内部逻辑：</p>
<pre><code class="language-java">public classPanedWindowProcessFunction{
    public BaseRow getWindowAggregationResult(W window) throws Exception {
        Iterable&lt;W&gt; panes = windowAssigner.splitIntoPanes(window);
        BaseRow acc = windowAggregator.createAccumulators();
        // null namespace means use heap data views
        windowAggregator.setAccumulators(null, acc);
        for (W pane : panes) {
            BaseRow paneAcc = ctx.getWindowAccumulators(pane);
            if (paneAcc != null) {
                windowAggregator.merge(pane, paneAcc);
            }
        }
        return windowAggregator.getValue(window);
    }
}
</code></pre>
<p><img src="_v_images/20210412125427840_804453196" alt="图片" /></p>
<h3 id="emittrigger触发器"><a class="header" href="#emittrigger触发器">Emit（Trigger）触发器</a></h3>
<ul>
<li>配置方式指定Trigger：Flink1.9.0目前支持通过TableConifg配置earlyFireInterval、lateFireInterval毫秒数，来指定窗口结束之前、窗口结束之后的触发策略（默认是watermark超过窗口结束后触发一次），策略的解析在WindowEmitStrategy，在StreamExecGroupWindowAggregateRule就会创建和解析这个策略</li>
<li>SQL方式指定Trigger：Flink1.9.0代码中calcite部分已有SqlEmit相关的实现，后续可以支持SQL 语句（INSERT INTO）中配置EMIT触发器</li>
</ul>
<p>本文Emit和Trigger都是触发器这一个概念，只是使用的方式不一样</p>
<p>1、Emit策略 Emit 策略是指在Flink SQL 中，query的输出策略（如能忍受的延迟）可能在不同的场景有不同的需求，而这部分需求，传统的 ANSI SQL 并没有对应的语法支持。比如用户需求：1小时的时间窗口，窗口触发之前希望每分钟都能看到最新的结果，窗口触发之后希望不丢失迟到一天内的数据。针对这类需求，抽象出了EMIT语法，并扩展到了SQL语法。</p>
<p>2、用途 EMIT语法的用途目前总结起来主要提供了：控制延迟、数据精确性，两方面的功能。</p>
<ul>
<li>控制延迟。针对大窗口，设置窗口触发之前的EMIT输出频率，减少用户看到结果的延迟(WITH| WITHOUT DELAY)。</li>
<li>数据精确性。不丢弃窗口触发之后的迟到的数据，修正输出结果(minIdleStateRetentionTime，在WindowEmitStrategy中生成allowLateness)。</li>
</ul>
<p>在选择EMIT策略时，还需要与处理开销进行权衡。因为越低的输出延迟、越高的数据精确性，都会带来越高的计算开销。</p>
<p>3、语法 EMIT 语法是用来定义输出的策略，即是定义在输出（INSERT INTO）上的动作。当未配置时，保持原有默认行为，即 window 只在 watermark 触发时 EMIT 一个结果。</p>
<p>语法：INSERT INTO tableName query EMIT strategy [, strategy]*</p>
<p>strategy ::= {WITH DELAY timeInterval | WITHOUT DELAY} [BEFORE WATERMARK |AFTER WATERMARK]</p>
<p>timeInterval ::=‘string’ timeUnit</p>
<p>WITH DELAY：声明能忍受的结果延迟，即按指定 interval 进行间隔输出。WITHOUT DELAY：声明不忍受延迟，即每来一条数据就进行输出。BEFORE WATERMARK：窗口结束之前的策略配置，即watermark 触发之前。AFTER WATERMARK：窗口结束之后的策略配置，即watermark 触发之后。注：</p>
<ul>
<li>其中 strategy可以定义多个，同时定义before和after的策略。但不能同时定义两个 before 或 两个after 的策略。</li>
<li>若配置了AFTER WATERMARK 策略，需要显式地在TableConfig中配置minIdleStateRetentionTime标识能忍受的最大迟到时间。</li>
<li>minIdleStateRetentionTime在window中只影响窗口何时清除，不直接影响窗口何时触发， 例如配置为3600000，最多容忍1小时的迟到数据，超过这个时间的数据会直接丢弃</li>
</ul>
<p>4、示例 如果我们已经有一个TUMBLE（ctime, INTERVAL ‘1’ HOUR）的窗口，tumble_window 的输出是需要等到一小时结束才能看到结果，我们希望能尽早能看到窗口的结果（即使是不完整的结果）。例如，我们希望每分钟看到最新的窗口结果：INSERT INTO result SELECT * FROM tumble_window EMIT WITH DELAY ‘1’ MINUTE BEFORE WATERMARK – 窗口结束之前，每隔1分钟输出一次更新结果</p>
<p>tumble_window 会忽略并丢弃窗口结束后到达的数据，而这部分数据对我们来说很重要，希望能统计进最终的结果里。而且我们知道我们的迟到数据不会太多，且迟到时间不会超过一天以上，并且希望收到迟到的数据立刻就更新结果：INSERT INTO result SELECT * FROM tumble_window EMIT WITH DELAY ‘1’ MINUTE BEFORE WATERMARK, WITHOUT DELAY AFTER WATERMARK --窗口结束之后，每条到达的数据都输出</p>
<p>tEnv.getConfig.setIdleStateRetentionTime(Time.days(1), Time.days(2))//min、max，只有Time.days(1)这个参数直接对window生效</p>
<p>补充一下WITH DELAY '1’这种配置的周期触发策略（即DELAY大于0），最后都是由ProcessingTime系统时间触发：</p>
<pre><code class="language-java">class WindowEmitStrategy{
  private def createTriggerFromInterval(
      enableDelayEmit: Boolean,
      interval: Long): Option[Trigger[TimeWindow]] = {
    if (!enableDelayEmit) {
      None
    } else {
      if (interval &gt; 0) {
       // 系统时间触发，小于wm的所有timer都执行onProcessingTime()
        Some(ProcessingTimeTriggers.every(Duration.ofMillis(interval)))
      } else {
       // 为0则每条都触发
        Some(ElementTriggers.every())
      }
    }
  }
}
</code></pre>
<p>5、Trigger类和结构关系 在源码中，Window Trigger的实现子类有10个左右，需要结合上一个小节的EMIT SQL能更容易理清他们之间的关系，这里简单介绍下：</p>
<p><img src="_v_images/20210412125427432_1763085070" alt="图片" /></p>
<ul>
<li>
<p>AfterEndOfWindow：这个就是没配置任何EMIT策略时，默认的EvenTime、ProcTime</p>
</li>
<li>
<p>Window触发策略（即窗口结束后触发一次）</p>
</li>
<li>
<p>EveryElement：即delay=0，在processElement()时直接触发，无论是在窗口结束之前或者窗口结束之后都触发，且不再注册timer</p>
</li>
<li>
<p>AfterEndOfWindowNoLate：对应EMIT WITHOUT DELAY AFTER WATERMARK，窗口结束之前不输出，窗口结束之后无延迟输出</p>
</li>
<li>
<p>AfterFirstElementPeriodic：对应WITH DELAY ‘1’ MINUTE BEFORE| AFTER WATERMARK，即按系统时间周期执行，由ProcessingTime系统时间周期触发</p>
</li>
</ul>
<hr />
<p>title: &quot;Flink-SQL原理之Calcite&quot;
layout: post
date: 2021-04-18 07:58:00
category: bigdata
tags:</p>
<ul>
<li>Flink</li>
<li>Calcite</li>
</ul>
<h2>share: true
comments: true</h2>
<blockquote>
<p>Flink SQL是Flink API的最顶层抽象，在使用Flink SQL的时候，是否被其便捷和高效惊艳到？到底一条SQL语句是如何运行起来的？提到Flink SQL，离不开SQL引擎框架 -- Calcite。Calcite是面向Hadoop的查询引擎，提供了SQL解析、优化、多重数据源查询的基础框架。Flink借助Calcite实现了SQL解析、优化和graph生成。</p>
</blockquote>
<h2 id="flink-api与flink-sql简介"><a class="header" href="#flink-api与flink-sql简介">Flink API与Flink SQL简介</a></h2>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418105609.png" alt="img" /></p>
<p>Flink提供了三层API抽象，每一层API都是便捷性和变现力之前的权衡，来应对不同的计算场景。</p>
<p>Flink SQL是最顶层的抽象，这层抽象在语义和程序表达式上都类似于Table API，但是程序实现都是SQL表达式: </p>
<ol>
<li>
<p>SQL和Table API都是遵循关系模型：元数据和操作</p>
<ul>
<li>元数据类似于关系型数据库中的schema</li>
<li>操作类似于关系型数据库中的操作, 如select、project、join、group-by和aggregate</li>
</ul>
</li>
<li>
<p>SQL和Table API都是声明式定义，而不会执行执行的具体代码</p>
</li>
<li>
<p>简洁，通过UDF扩展，但比core API的表达能力差</p>
</li>
<li>
<p>执行之前，通过优化器中的优化规则对用户编写的表达式进行优化</p>
</li>
<li>
<p>TableEnvironment是Flink SQL和Table API的入口，可以无缝切换到DataStream/DataSet, 允许混用。</p>
</li>
</ol>
<h2 id="calcite"><a class="header" href="#calcite">Calcite</a></h2>
<h3 id="calcite简介"><a class="header" href="#calcite简介">Calcite简介</a></h3>
<p>是一个动态数据的管理框架，可以用来构建数据库系统的语法解析模块</p>
<ul>
<li>不包含数据存储、数据处理等功能</li>
<li>可以通过编写 Adaptor 来扩展功能，以支持不同的数据处理平台</li>
<li>Flink SQL 使用并对其扩展以支持 SQL 语句的解析和验证</li>
</ul>
<p>Calcite提供了SQL parser、SQL validation、Query optimizer、SQL generator和Data federator</p>
<h3 id="查询的执行过程"><a class="header" href="#查询的执行过程">查询的执行过程</a></h3>
<p>分四步: </p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418225117.png" alt="img" /></p>
<ol>
<li>Parse(SQL -&gt; SqlNode ):  使用JavaCC生成的parser来转换查询</li>
<li>validate(SqlNode -&gt; SqlNode ): 通过元数据验证查询</li>
<li>optimize: 逻辑计划优化和转换为物理计划
<ul>
<li>语义分析(SqlNode -&gt; RelNode/RexNode ): 根据 SqlNode 及元信息构建 RelNode 树，也就是最初版本的逻辑计划（Logical Plan）；</li>
<li>逻辑计划优化(RelNode -&gt; RelNode ): 优化器的核心，根据前面生成的逻辑计划按照相应的规则（Rule）进行优化；</li>
</ul>
</li>
<li>execute: 物理计划转换为应用框架的执行逻辑(如Flink的graph)</li>
</ol>
<h3 id="catalog"><a class="header" href="#catalog">Catalog</a></h3>
<p>定义Calcite查询的命名空间:</p>
<p><code>Schema</code> :  <code>schema</code>和<code>table</code>的集合，可以任意嵌套</p>
<p><code>Table</code>: 代表单个数据集，字段定义为<code>RelDataType</code></p>
<p><code>RelDataType</code>:  代表数据集中的字段，支持所有的SQL数据类型，包括结构体</p>
<p><code>Statistic</code>: 提供用于优化的表统计信息, 如行数、分布信息、是否为key</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418173928.png" alt="image-20210418173928391" /></p>
<h3 id="sql-parser"><a class="header" href="#sql-parser">SQL parser</a></h3>
<ul>
<li>LL(K) parser通过JavaCC(Java Compiler Compiler)写的</li>
<li>输入的查询转换为AST(abstract syntax tree)</li>
<li><code>SqlNode</code>代表Token</li>
<li><code>SqlNode</code>可以通过<code>unparse</code>方法转换会SQL</li>
</ul>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418181320.png" alt="image-20210418181320343" /></p>
<p><code>SqlNode</code>代表AST中的一个节点</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418181402.png" alt="image-20210418181402108" /></p>
<p><code>SqlDialect</code>代表特定数据库的方言规则</p>
<h3 id="query-optimizer"><a class="header" href="#query-optimizer">Query optimizer</a></h3>
<p>查询计划(Query plans)代表执行一个查询必须的步骤</p>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418184009.png" alt="image-20210418184009836" /></p>
<p>Query优化:</p>
<ul>
<li>优化逻辑计划</li>
<li>目标通常是尽量减少计划中必须在早期处理的数据量</li>
<li>将逻辑计划转换为物理计划</li>
<li>物理计划与引擎有关，代表了物理执行过程</li>
</ul>
<p>常见的优化方法:</p>
<table><thead><tr><th><strong>RBO</strong></th><th><strong>规则名称</strong></th><th></th></tr></thead><tbody>
<tr><td>列裁剪</td><td>column_prune</td><td>Prune unused fields</td></tr>
<tr><td>子查询去关联</td><td>decorrelate</td><td></td></tr>
<tr><td>子查询转换为join</td><td></td><td>Convert subqueries to joins</td></tr>
<tr><td>聚合消除</td><td>aggregation_eliminate</td><td></td></tr>
<tr><td>投影消除</td><td>projection_eliminate</td><td></td></tr>
<tr><td>最大最小消除</td><td>max_min_eliminate</td><td></td></tr>
<tr><td>谓词下推</td><td>predicate_push_down</td><td></td></tr>
<tr><td>外连接消除</td><td>outer_join_eliminate</td><td></td></tr>
<tr><td>分区裁剪</td><td>partition_processor</td><td></td></tr>
<tr><td>聚合下推</td><td>aggregation_push_down</td><td></td></tr>
<tr><td>TopN 下推</td><td>topn_push_down</td><td></td></tr>
<tr><td>Join 重排序</td><td>join_reorder</td><td></td></tr>
</tbody></table>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418190336.png" alt="image-20210418190335990" /></p>
<h3 id="核心概念"><a class="header" href="#核心概念">核心概念</a></h3>
<ol>
<li><strong>关系代数</strong>（Relational algebra）：即关系表达式。它们通常以动词命名，例如 Sort, Join, Project, Filter, Scan, Sample.</li>
<li><strong>行表达式</strong>（Row expressions）：例如 RexLiteral (常量), RexVariable (变量), RexCall (调用) 等，例如投影列表（Project）、过滤规则列表（Filter）、JOIN 条件列表和 ORDER BY 列表、WINDOW 表达式、函数调用等。使用 RexBuilder 来构建行表达式。</li>
<li>表达式有各种<strong>特征</strong>（Trait）：使用 Trait 的 satisfies() 方法来测试某个表达式是否符合某 Trait 或 Convention.</li>
<li><strong>转化特征</strong>（Convention）：属于 Trait 的子类，用于转化 RelNode 到具体平台实现（可以将下文提到的 Planner 注册到 Convention 中）. 例如 JdbcConvention，FlinkConventions.DATASTREAM 等。同一个关系表达式的输入必须来自单个数据源，各表达式之间通过 Converter 生成的 Bridge 来连接。</li>
<li><strong>规则</strong>（Rules）：用于将一个表达式转换（Transform）为另一个表达式。它有一个由 RelOptRuleOperand 组成的列表来决定是否可将规则应用于树的某部分。</li>
</ol>
<h3 id="planner规划器"><a class="header" href="#planner规划器">Planner(规划器)</a></h3>
<p><strong>规划器</strong>（Planner） ：即请求优化器，它可以根据一系列规则和成本模型（例如基于成本的优化模型 VolcanoPlanner、启发式优化模型 HepPlanner）来将一个表达式转为语义等价（但效率更优）的另一个表达式。</p>
<h4 id="hepplanner启发式优化模型"><a class="header" href="#hepplanner启发式优化模型">HepPlanner(启发式优化模型)</a></h4>
<ul>
<li>与Spark优化器类似的启发式优化器</li>
<li>启发式优化比CBO要快速</li>
<li>如果规则对计划做出相反的改变，则存在无限递归的风险</li>
</ul>
<h4 id=""><a class="header" href="#"></a></h4>
<h4 id="volcanoplanner基于成本的优化模型"><a class="header" href="#volcanoplanner基于成本的优化模型">VolcanoPlanner(基于成本的优化模型)</a></h4>
<ul>
<li>遍历所有的规则，选择代价最小的计划</li>
<li>代价是通过关系表达式提供的</li>
<li>并不是所有可能的计划都会计算</li>
<li>当经过指定的迭代未显著提升将停止优化</li>
<li>代价包括行数、I/O和CPU</li>
<li>Statistics用来提高代价评估的准确性</li>
<li>Calcite提供了工具来在统计资源消耗</li>
</ul>
<h2 id="flink-与-calcite"><a class="header" href="#flink-与-calcite">Flink 与 Calcite</a></h2>
<h3 id="calcite中flink中的重要作用"><a class="header" href="#calcite中flink中的重要作用">Calcite中Flink中的重要作用</a></h3>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418224641.png" alt="在这里插入图片描述" /></p>
<p>在Flink中，Calcite扮演着重要的角色：</p>
<ul>
<li>以Calcite Catalog为核心，上面承载了Table/SQL API</li>
<li>Flink SQL和Table的代码最后生成Calcite Logic Plan(SqlNode Tree)</li>
<li>随后验证、优化为 RelNode 树，</li>
<li>最终通过 Rules（规则）和 Convention（转化特征）生成具体的 DataSet Plan（批处理）或 DataStream Plan（流处理），即 Flink 算子构成的处理逻辑。</li>
</ul>
<p><img src="https://gitee.com/averyzhang/pic-go/raw/master/img/20210418225006.png" alt="在这里插入图片描述" /></p>
<p>Table / SQL API 的编程框架如下：</p>
<ul>
<li>
<p>通过 TableEnvironment 配置 CalciteConfig 对象，自动设置 SQL &amp; Table API 默认处理参数。</p>
</li>
<li>
<p>使用 registerTableSource() 来将一个 TableSource 注册到 rootSchema. 后续可以通过 scan() 获取此 Table 并调用各种 Table API 进行处理。</p>
</li>
<li>
<p>接下可以调用 sqlQuery() 和 sqlUpdate() 方法来使用 SQL 语句进行数据处理。</p>
</li>
</ul>
<h3 id="flink-sql的执行流程"><a class="header" href="#flink-sql的执行流程">Flink SQL的执行流程</a></h3>
<p><code>Planner接口</code>: 解析SQL，转换为Transformation</p>
<p><code>Executor接口</code>: 将Planner转换的Transformation生成streamGraph并执行</p>
<p><code>Parser接口</code>: 负责SQL解析，parse方法将SQL语句转换为Operation数组</p>
<ul>
<li>通过Calcite将SQL解析为SqlNode</li>
<li>根据SqlNode的类型，将SqlNode转换为Operation数组</li>
</ul>
<p>DDL语句的转换过程:</p>
<p>SqlNode转换为RelNode:</p>
<ul>
<li>推断Table类型</li>
<li>推断计算列</li>
<li>推断watermark分配</li>
</ul>
<p>SQL转换:</p>
<ol>
<li>Operation-&gt;RelNode</li>
<li>优化RelNode</li>
<li>RelNode-&gt;ExecNode</li>
<li>ExecNode-&gt;Transformation算子</li>
</ol>
<p>[参考文献]</p>
<ol>
<li>[Parsing database Query with Apache Calcite](<a href="https://blog.knoldus.com/parsing-database-query-with-apache-calcite/">Parsing database Query with Apache Calcite - Knoldus Blogs</a>)</li>
<li><a href="https://www.slideshare.net/julianhyde/apache-calcite-one-planner-fits-all">Apache Calcite: One planner fits all (slideshare.net)</a></li>
</ol>
<h1 id="flink-sql-代码生成"><a class="header" href="#flink-sql-代码生成">Flink SQL 代码生成</a></h1>
<p><a href="https://www.cnblogs.com/rossiXYZ/p/12773123.html">从&quot;UDF不应有状态&quot; 切入来剖析Flink SQL代码生成</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
